{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3b7e39-cfb2-4306-8b79-6b04be459d35",
   "metadata": {},
   "source": [
    "# Majority vote model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52649dc-4a4e-4749-9bf8-12d9aea8d4be",
   "metadata": {},
   "source": [
    "__Objective:__ develop a model for toxicity prediction on text trained with labels aggregated over annotators by majority vote (**no annotator modelling**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3823f1f0-86ac-4d2e-8909-96a4d782f46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (AutoConfig, PretrainedConfig, AutoTokenizer, RobertaForSequenceClassification,\n",
    "    pipeline)\n",
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n",
    "import datasets\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from custom_logger import get_logger\n",
    "from model_utils import freeze_model_weights\n",
    "\n",
    "logger = get_logger('majority_vote_fine_tuning')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60deb0-e5ef-4887-aaf9-d52adf642ff0",
   "metadata": {},
   "source": [
    "## Load data and aggregate labels by majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a022f3-f00c-4319-a6ac-804c5fc4b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aggregated_labels_dataset(dataset_name, dataset_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(dataset_path)\n",
    "\n",
    "    if dataset_name.lower() == 'popquorn':\n",
    "        data_df = pd.merge(\n",
    "            left=data_df[['instance_id', 'text']].drop_duplicates(subset='instance_id'),\n",
    "            right=data_df.groupby('instance_id').apply(\n",
    "                lambda group: group['offensiveness'].value_counts().sort_values(ascending=False).index[0]\n",
    "            ).reset_index().rename(columns={0: 'offensiveness'}),\n",
    "            on='instance_id',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        data_df['offensiveness'] = data_df['offensiveness'].astype(int)\n",
    "\n",
    "        data_df['label'] = (data_df['offensiveness'] - 1).astype(int)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset_name} not supported')\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1980fb4-77a2-463f-9e7f-fbc7498738df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "POPQUORN_DATA_PATH = '../data/samples/POPQUORN_offensiveness.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c1f3d0-f272-4b73-8641-a69af337892e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2745216/2054353083.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  right=data_df.groupby('instance_id').apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>text</th>\n",
       "      <th>offensiveness</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530</td>\n",
       "      <td>I think a lot of Dethklok songs use drop C, wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1280</td>\n",
       "      <td>There are relatively simple ways around all of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621</td>\n",
       "      <td>Tell the british soldier in WW1 to shoot that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>676</td>\n",
       "      <td>Top comment pretty much. I have gay friends an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>635</td>\n",
       "      <td>Don't tell them just let them and their liniag...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1217</td>\n",
       "      <td>My six year old gets to a state where he's abs...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>415</td>\n",
       "      <td>march 14, the Little Dipper was missing... any...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>173</td>\n",
       "      <td>But by the same token that logic would apply t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>938</td>\n",
       "      <td>As soon as her all expenses paid trip to Epste...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1151</td>\n",
       "      <td>Woman's studies is a pretty ridiculous field.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instance_id                                               text  \\\n",
       "0             530  I think a lot of Dethklok songs use drop C, wo...   \n",
       "1            1280  There are relatively simple ways around all of...   \n",
       "2             621  Tell the british soldier in WW1 to shoot that ...   \n",
       "3             676  Top comment pretty much. I have gay friends an...   \n",
       "4             635  Don't tell them just let them and their liniag...   \n",
       "...           ...                                                ...   \n",
       "1495         1217  My six year old gets to a state where he's abs...   \n",
       "1496          415  march 14, the Little Dipper was missing... any...   \n",
       "1497          173  But by the same token that logic would apply t...   \n",
       "1498          938  As soon as her all expenses paid trip to Epste...   \n",
       "1499         1151      Woman's studies is a pretty ridiculous field.   \n",
       "\n",
       "      offensiveness  label  \n",
       "0                 1      0  \n",
       "1                 1      0  \n",
       "2                 1      0  \n",
       "3                 1      0  \n",
       "4                 4      3  \n",
       "...             ...    ...  \n",
       "1495              1      0  \n",
       "1496              1      0  \n",
       "1497              1      0  \n",
       "1498              1      0  \n",
       "1499              3      2  \n",
       "\n",
       "[1500 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_vote_data_df = generate_aggregated_labels_dataset('popquorn', POPQUORN_DATA_PATH)\n",
    "\n",
    "majority_vote_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0735bc-1d54-4bfb-baad-511a01d764ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_vote_data_df['offensiveness'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e088dc-d965-40e1-9632-2d0b11e207c7",
   "metadata": {},
   "source": [
    "Train-test split and casting into Hugging Face datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bad0ba0-be7a-434c-9ded-0ca187679f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:51:49,055 - majority_vote_fine_tuning - INFO - Splitting training and test dataset (test_frac: 0.25)\n"
     ]
    }
   ],
   "source": [
    "test_frac = 0.25\n",
    "\n",
    "logger.info(f'Splitting training and test dataset (test_frac: {test_frac})')\n",
    "\n",
    "majority_vote_data_df.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "test_data = majority_vote_data_df.iloc[-int(len(majority_vote_data_df) * test_frac):]\n",
    "training_data = majority_vote_data_df[~majority_vote_data_df.index.isin(test_data.index)]\n",
    "\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Check.\n",
    "assert len(set(test_data['instance_id']) & set(training_data['instance_id'])) == 0\n",
    "\n",
    "# # Format dataset.\n",
    "# dataset = {\n",
    "#     'train': training_data[['text', 'label']].to_dict(orient='records'),\n",
    "#     'test': test_data[['text', 'label']].to_dict(orient='records'),\n",
    "# }\n",
    "\n",
    "# dataset['train'] = [dict(tokenize_function(sample), **{'label': sample['label'], 'text': sample['text']}) for sample in dataset['train']]\n",
    "# dataset['test'] = [dict(tokenize_function(sample), **{'label': sample['label'], 'text': sample['text']}) for sample in dataset['test']]\n",
    "train_ds = datasets.Dataset.from_dict(training_data.drop(columns=['instance_id', 'offensiveness']).to_dict(orient='list'))\n",
    "test_ds = datasets.Dataset.from_dict(test_data.drop(columns=['instance_id', 'offensiveness']).to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d305852-94f3-4c6b-9a3d-03889db62504",
   "metadata": {},
   "source": [
    "## Load a RoBERTa-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f08d3c6-9125-48d2-8be6-0019e11496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return roberta_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        # return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf6556-7d61-4380-97fc-6fb5212ecc5b",
   "metadata": {},
   "source": [
    "Pretrained encoder, newly initialized classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c6815f-0ebe-4532-8b4d-d873519f0eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ice4hpc/rel-24.04/conda/envs/huggingface/lib/python3.11/site-packages/huggingface_hub-0.24.0-py3.8.egg/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b22e2707f045d7bf6de3813197e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'roberta-base'\n",
    "\n",
    "num_labels = majority_vote_data_df['label'].unique().shape[0]\n",
    "\n",
    "# # Config for the encoder.\n",
    "roberta_classifier_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir='/data/milanlp/huggingface/hub/',\n",
    "    finetuning_task=\"text-classification\",\n",
    "    id2label={\n",
    "        i: label\n",
    "        for i, label in enumerate(range(num_labels))\n",
    "    },\n",
    "    label2id={\n",
    "        label: i\n",
    "        for i, label in enumerate(range(num_labels))\n",
    "    }\n",
    "    # id2label={\n",
    "    #     i: int(label)\n",
    "    #     for i, label in enumerate(majority_vote_data_df['offensiveness'].unique())\n",
    "    # },\n",
    "    # label2id={\n",
    "    #     int(label): i\n",
    "    #     for i, label in enumerate(majority_vote_data_df['offensiveness'].unique())\n",
    "    # }\n",
    ")\n",
    "\n",
    "# Config for the classification head. These are all the\n",
    "# parameters a `RobertaClassificationHead` requires.\n",
    "roberta_classification_head_config = PretrainedConfig()\n",
    "\n",
    "roberta_classification_head_config.classifier_dropout = 0.1\n",
    "roberta_classification_head_config.hidden_size = 64\n",
    "roberta_classification_head_config.num_labels = majority_vote_data_df['offensiveness'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715bd2a6-b891-4057-9a6a-aae8b51990c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:57:07,854 - majority_vote_fine_tuning - INFO - Instantiating tokenizer, classification model and pipeline\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Instantiating tokenizer, classification model and pipeline')\n",
    "\n",
    "# Instantiate tokenizer.\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir='/data/milanlp/huggingface/hub/',\n",
    ")\n",
    "\n",
    "# Instantiate RoBERTa model.\n",
    "roberta_classifier = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    cache_dir='/data/milanlp/huggingface/hub/',\n",
    "    config=roberta_classifier_config,\n",
    ")\n",
    "\n",
    "# Substitute the default classification head with a custom one.\n",
    "classification_head = RobertaClassificationHead(roberta_classification_head_config)\n",
    "classification_head.dense = torch.nn.Linear(\n",
    "    roberta_classifier.config.hidden_size,  # The `in_features` parameter must be equal to the encoder's hidden size.\n",
    "    roberta_classification_head_config.hidden_size,\n",
    ")\n",
    "\n",
    "roberta_classifier.classifier = classification_head\n",
    "\n",
    "\n",
    "# Put everything together in a single pipeline object.\n",
    "roberta_classifier_pipeline = pipeline(\n",
    "    task='text-classification',\n",
    "    config=roberta_classifier_config,\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    model=roberta_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fd6f3eb-0c20-4a1a-a135-92f416b1ffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.8281), logits=tensor([[-0.2774, -0.1361,  0.0634,  0.1740, -0.1138],\n",
       "        [-0.2138, -0.1404,  0.0487,  0.3174, -0.1287],\n",
       "        [-0.1339, -0.1219, -0.0147,  0.3481, -0.1053],\n",
       "        [-0.3116, -0.1938,  0.0985,  0.2401, -0.0562]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = roberta_classifier(**dict(\n",
    "        **roberta_tokenizer(\n",
    "            majority_vote_data_df['text'].iloc[:4].tolist(), return_tensors='pt', padding=True\n",
    "        ),#.to(device='cuda:0'),\n",
    "        **{'labels': torch.LongTensor(majority_vote_data_df['label'].iloc[:4])}#.to(device='cuda:0')}\n",
    "    ))\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fb3e3-4c6f-4701-b12c-dfbd171dcf7e",
   "metadata": {},
   "source": [
    "Tokenize datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55675070-1bc8-45f6-b3bb-0938515ef6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:16:50,361 - majority_vote_fine_tuning - INFO - Tokenizing datasets\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1125/1125 [00:00<00:00, 6695.32 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [00:00<00:00, 6751.62 examples/s]\n",
      "2024-12-02 13:16:50,634 - majority_vote_fine_tuning - INFO - Training dataset size: 1125 | Test dataset size: 375\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets.\n",
    "logger.info(f'Tokenizing datasets')\n",
    "\n",
    "tokenized_train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "logger.info(f'Training dataset size: {len(train_ds)} | Test dataset size: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1e924-720f-4f5f-a776-0bdd03d535cc",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac446bc-c515-4379-a0b2-ef0806412c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metrics = evaluate.load('accuracy')\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     # print(eval_pred.__dict__)\n",
    "    \n",
    "#     predictions = np.argmax(eval_pred.logits, axis=1)\n",
    "\n",
    "#     return metrics.compute(\n",
    "#         predictions=predictions, references=eval_pred.label_ids\n",
    "#     )\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    \n",
    "    return metrics.compute(\n",
    "        predictions=predictions, references=eval_pred.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91276cf-c4f6-404a-ab66-c97e8f6d45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:16:51,482 - majority_vote_fine_tuning - INFO - Module: roberta | N parameters: 124055040 | Parameters trainable: False | Training mode: False\n",
      "2024-12-02 13:16:51,483 - majority_vote_fine_tuning - INFO - Module: classifier | N parameters: 49541 | Parameters trainable: True | Training mode: True\n",
      "2024-12-02 13:16:51,484 - majority_vote_fine_tuning - INFO - N params: 124104581 | N trainable params: 49541\n"
     ]
    }
   ],
   "source": [
    "FREEZE_ENCODER_PARAMS = True\n",
    "\n",
    "if FREEZE_ENCODER_PARAMS:\n",
    "    freeze_model_weights(roberta_classifier_pipeline.model, trainable_modules=['classifier'])\n",
    "\n",
    "n_params_total = sum([p.numel() for p in roberta_classifier_pipeline.model.parameters()])\n",
    "n_params_trainable = sum([p.numel() for p in roberta_classifier_pipeline.model.parameters() if p.requires_grad])\n",
    "\n",
    "logger.info(\n",
    "    f'N params: {n_params_total} | N trainable params: {n_params_trainable}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8869c869-d01a-4eb3-8d13-a016abe2287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT_DIR = '/data1/moscato/personalised-hate-boundaries-data/models/'\n",
    "N_EPOCHS = 5\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  # Options: 'no', 'epoch', 'steps' (requires the `save_steps` argument to be set though.\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,  # Default: 8.\n",
    "    gradient_accumulation_steps=1,  # Default: 1.\n",
    "    per_device_eval_batch_size=8,  # Default: 8.\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    # label_names=list(roberta_classifier.config.id2label.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12d6134-63a1-46f2-b235-a45e0786c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=roberta_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # tokenizer=roberta_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb36fc9-0f86-4b89-ac59-20362e1c31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='355' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [355/355 01:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.306200</td>\n",
       "      <td>1.229262</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.031400</td>\n",
       "      <td>0.977673</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.917813</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.935600</td>\n",
       "      <td>0.906314</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>0.903759</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=355, training_loss=1.0571492410041916, metrics={'train_runtime': 72.3508, 'train_samples_per_second': 77.746, 'train_steps_per_second': 4.907, 'total_flos': 1470623748480000.0, 'train_loss': 1.0571492410041916, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
