{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_DIR = '/data1/moscato/personalised-hate-boundaries-data/data/final_predictions/'\n",
    "\n",
    "if not os.path.exists(\"final_predictions/kumar_majority_vote_predictions_on_personalizedkumar.csv\"):\n",
    "    # Read the predictions from the three models\n",
    "    # kumar_maj = pd.read_csv(\"final_predictions/kumar_majority_vote_predictions_on_kumar.csv\")\n",
    "    # kumar_sep = pd.read_csv(\"final_predictions/kumar_sepheads_predictions_on_kumar.csv\")\n",
    "    # mhs_maj = pd.read_csv(\"final_predictions/mhs_majority_vote_predictions_on_mhs.csv\")\n",
    "    # mhs_sep = pd.read_csv(\"final_predictions/mhs_sepheads_predictions_on_mhs.csv\")\n",
    "    kumar_maj = pd.read_csv(os.path.join(PREDICTIONS_DIR, \"kumar_majority_vote_predictions_on_kumar.csv\"))\n",
    "    kumar_sep = pd.read_csv(os.path.join(PREDICTIONS_DIR, \"kumar_sepheads_predictions_on_kumar.csv\"))\n",
    "    mhs_maj = pd.read_csv(os.path.join(PREDICTIONS_DIR, \"mhs_majority_vote_predictions_on_mhs.csv\"))\n",
    "    mhs_sep = pd.read_csv(os.path.join(PREDICTIONS_DIR, \"mhs_sepheads_predictions_on_mhs.csv\"))\n",
    "\n",
    "    # Merge the predictions on text_id\n",
    "    # kumar_maj = kumar_maj.merge(kumar_sep, on='text_id', suffixes=('_maj', '_sep'))\n",
    "    kumar_maj = pd.merge(\n",
    "        left=kumar_sep,\n",
    "        right=kumar_maj[['text_id', 'majority_vote_model_predicted_toxic_score']],\n",
    "        how='left',\n",
    "        on='text_id'\n",
    "    )\n",
    "    \n",
    "    # mhs_maj = mhs_maj.merge(mhs_sep, on='text_id', suffixes=('_maj', '_sep'))\n",
    "    mhs_maj = pd.merge(\n",
    "        left=mhs_sep,\n",
    "        right=mhs_maj[['text_id', 'majority_vote_model_predicted_toxic_score']],\n",
    "        how='left',\n",
    "        on='text_id'\n",
    "    )\n",
    "\n",
    "    # Save the merged predictions\n",
    "    # kumar_maj.to_csv(\"final_predictions/kumar_majority_vote_predictions_on_personalizedkumar.csv\", index=False)\n",
    "    # mhs_maj.to_csv(\"final_predictions/kumar_sepheads_predictions_on_personalizedmhs.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kumar_majority_on_kumar\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_predictions/kumar_majority_vote_predictions_on_personalizedkumar.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_name)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhatecheck\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     results_df[dataset][model_type][eval_set], f1, breakdown, confusion_matrix[dataset][model_type][eval_set] \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1,\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextreme_annotator_0_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: breakdown\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextreme_annotator_1_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: breakdown\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m     })\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mload_and_evaluate\u001b[0;34m(path, model_type, eval_set, average_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m prediction_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_predictions/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m breakdown_confusion_matrix \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 28\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmajority\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     preds \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmajority_vote_model_predicted_toxic_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_predictions/kumar_majority_vote_predictions_on_personalizedkumar.csv'"
     ]
    }
   ],
   "source": [
    "# Filenames\n",
    "files = {\n",
    "    \"kumar\": {\n",
    "        \"majority\": {\n",
    "            \"kumar\": \"kumar_majority_vote_predictions_on_personalizedkumar.csv\",\n",
    "            \"hatecheck\": \"kumar_majority_vote_predictions_on_hatecheck.csv\",\n",
    "        },\n",
    "        \"sepheads\": {\n",
    "            \"kumar\": \"kumar_sepheads_predictions_on_kumar.csv\",\n",
    "            \"hatecheck\": \"kumar_sepheads_predictions_on_hatecheck.csv\",\n",
    "        }\n",
    "    },\n",
    "    \"mhs\": {\n",
    "        \"majority\": {\n",
    "            \"mhs\": \"kumar_sepheads_predictions_on_personalizedmhs.csv\",\n",
    "            \"hatecheck\": \"mhs_majority_vote_predictions_on_hatecheck.csv\",\n",
    "        },\n",
    "        \"sepheads\": {\n",
    "            \"mhs\": \"mhs_sepheads_predictions_on_mhs.csv\",\n",
    "            \"hatecheck\": \"mhs_sepheads_predictions_on_hatecheck.csv\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_and_evaluate(path, model_type, eval_set, average_type=\"macro\"):\n",
    "    prediction_path = 'final_predictions/'\n",
    "    breakdown_confusion_matrix = {}\n",
    "    df = pd.read_csv(prediction_path+path)\n",
    "    if model_type == \"majority\":\n",
    "        preds = df[\"majority_vote_model_predicted_toxic_score\"]\n",
    "    else:  # sepheads\n",
    "        preds = df[\"sepheads_predicted_toxic_score\"]\n",
    "\n",
    "    if eval_set == \"hatecheck\":\n",
    "        df[\"toxic_score\"] = df[\"label_gold\"]\n",
    "        y_true = df[\"toxic_score\"]\n",
    "    else:\n",
    "        if model_type == \"majority\":\n",
    "            df[\"toxic_score\"] = df[\"toxic_score_sep\"]\n",
    "        y_true = df[\"toxic_score\"]\n",
    "\n",
    "    f1_macro = f1_score(y_true, preds, average=average_type)\n",
    "    # compute confusion matrix\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_true, preds)\n",
    "    breakdown_confusion_matrix['all'] = confusion_matrix\n",
    "    print(classification_report(y_true, preds, output_dict=True))\n",
    "\n",
    "    if eval_set != \"hatecheck\":\n",
    "\n",
    "        # # Create a dictionary to hold F1 scores per annotator\n",
    "        # f1_per_annotator = {}\n",
    "\n",
    "        # # Group by annotator_id\n",
    "        # for annotator_id, group in df.groupby(\"annotator_id\"):\n",
    "        #     y_true = group[\"toxic_score\"]\n",
    "        #     y_pred = group[\"boundary_model_predicted_toxic_score\"]\n",
    "            \n",
    "        #     # If there's only one class in y_true or y_pred, f1_score can throw a warning or error\n",
    "        #     if len(set(y_true)) > 1 or len(set(y_pred)) > 1:\n",
    "        #         f1 = f1_score(y_true, y_pred)\n",
    "        #     else:\n",
    "        #         f1 = 0.0  # Can't compute F1 if there's no variation\n",
    "        #         print(\"PROBLEM!\")\n",
    "            \n",
    "        #     f1_per_annotator[annotator_id] = f1\n",
    "        \n",
    "        # average_f1 = sum(f1_per_annotator.values()) / len(f1_per_annotator)\n",
    "        # print(average_f1)\n",
    "\n",
    "\n",
    "        f1_breakdown = df.groupby(\"extreme_annotator\").apply(\n",
    "            lambda g: f1_score(g[\"toxic_score\"], g[\"sepheads_predicted_toxic_score\"], average=average_type)\n",
    "        ).to_dict()\n",
    "        breakdown_confusion_matrix[0] = sklearn.metrics.confusion_matrix(\n",
    "            df[df[\"extreme_annotator\"] == False][\"toxic_score\"],\n",
    "            df[df[\"extreme_annotator\"] == False][\"sepheads_predicted_toxic_score\"]\n",
    "        )\n",
    "        breakdown_confusion_matrix[1] = sklearn.metrics.confusion_matrix(\n",
    "            df[df[\"extreme_annotator\"] == True][\"toxic_score\"],\n",
    "            df[df[\"extreme_annotator\"] == True][\"sepheads_predicted_toxic_score\"]\n",
    "        )\n",
    "\n",
    "        return df, f1_macro, f1_breakdown, breakdown_confusion_matrix\n",
    "\n",
    "    return df, f1_macro, confusion_matrix\n",
    "\n",
    "average_type = \"macro\"  # Change to \"macro\" for macro F1\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "results_df = {\n",
    "    \"kumar\": {\n",
    "        \"majority\": {},\n",
    "        \"sepheads\": {}\n",
    "    },\n",
    "    \"mhs\": {\n",
    "        \"majority\": {},\n",
    "        \"sepheads\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "confusion_matrix = {\n",
    "    \"kumar\": {\n",
    "        \"majority\": {\n",
    "            \"kumar\" : {},\n",
    "            \"hatecheck\" : {}\n",
    "        },\n",
    "        \"sepheads\": {\n",
    "            \"kumar\" : {},\n",
    "            \"hatecheck\" : {}\n",
    "        }\n",
    "    },\n",
    "    \"mhs\": {\n",
    "        \"majority\": {\n",
    "            \"mhs\" : {},\n",
    "            \"hatecheck\" : {}\n",
    "        },\n",
    "        \"sepheads\": {\n",
    "            \"mhs\" : {},\n",
    "            \"hatecheck\" : {}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for dataset, models in files.items():\n",
    "    for model_type, sources in models.items():\n",
    "        for eval_set, file_path in sources.items():\n",
    "            for average_type in [\"macro\", \"positive\"]:\n",
    "                model_name = f\"{dataset}_{model_type}_on_{eval_set}\"\n",
    "                print(model_name)\n",
    "                if eval_set != \"hatecheck\":\n",
    "                    results_df[dataset][model_type][eval_set], f1, breakdown, confusion_matrix[dataset][model_type][eval_set] = load_and_evaluate(file_path, model_type, eval_set, average_type)\n",
    "                    results.append({\n",
    "                        \"model\": model_name,\n",
    "                        \"macro_f1\": f1,\n",
    "                        \"extreme_annotator_0_f1\": breakdown.get(False),\n",
    "                        \"extreme_annotator_1_f1\": breakdown.get(True)\n",
    "                    })\n",
    "                else:\n",
    "                    results_df[dataset][model_type][eval_set],f1, confusion_matrix[dataset][model_type][eval_set]['all'] = load_and_evaluate(file_path, model_type, eval_set, average_type)\n",
    "                    results.append({\n",
    "                        \"model\": model_name,\n",
    "                        \"macro_f1\": f1\n",
    "                    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_scores = pd.DataFrame(results)\n",
    "df_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19973 15474]\n",
      "[0.56346094 0.43653906]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9300e_row0_col0 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_9300e_row0_col1 {\n",
       "  background-color: #d5e5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_9300e_row1_col0 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_9300e_row1_col1 {\n",
       "  background-color: #3585bf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9300e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9300e_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_9300e_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9300e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9300e_row0_col0\" class=\"data row0 col0\" >13720</td>\n",
       "      <td id=\"T_9300e_row0_col1\" class=\"data row0 col1\" >6253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9300e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9300e_row1_col0\" class=\"data row1 col0\" >4684</td>\n",
       "      <td id=\"T_9300e_row1_col1\" class=\"data row1 col1\" >10790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe0f00bc640>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(confusion_matrix[\"kumar\"][\"majority\"][\"kumar\"]['all'].sum(axis=1))\n",
    "# get percentage\n",
    "print(confusion_matrix[\"kumar\"][\"majority\"][\"kumar\"]['all'].sum(axis=1) / (confusion_matrix[\"kumar\"][\"majority\"][\"kumar\"]['all'].sum()))\n",
    "pd.DataFrame(confusion_matrix[\"kumar\"][\"majority\"][\"kumar\"]['all']).style.background_gradient(cmap='Blues', axis=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19973 15474]\n",
      "[0.56346094 0.43653906]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_442f9_row0_col0 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_442f9_row0_col1 {\n",
       "  background-color: #ebf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_442f9_row1_col0 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_442f9_row1_col1 {\n",
       "  background-color: #3989c1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_442f9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_442f9_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_442f9_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_442f9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_442f9_row0_col0\" class=\"data row0 col0\" >15184</td>\n",
       "      <td id=\"T_442f9_row0_col1\" class=\"data row0 col1\" >4789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_442f9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_442f9_row1_col0\" class=\"data row1 col0\" >4099</td>\n",
       "      <td id=\"T_442f9_row1_col1\" class=\"data row1 col1\" >11375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe0efffebe0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"]['all'].sum(axis=1))\n",
    "# get percentage\n",
    "print(confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"]['all'].sum(axis=1) / (confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"]['all'].sum()))\n",
    "pd.DataFrame(confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"]['all']).style.background_gradient(cmap='Blues', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds VS Kumar annotator-level labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14095  5878]\n",
      " [ 4986 10488]]\n",
      "[0.56346094 0.43653906]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.739     0.706     0.722     19973\n",
      "           1      0.641     0.678     0.659     15474\n",
      "\n",
      "    accuracy                          0.694     35447\n",
      "   macro avg      0.690     0.692     0.690     35447\n",
      "weighted avg      0.696     0.694     0.694     35447\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.736     0.711     0.723     19498\n",
      "           1      0.648     0.676     0.662     15347\n",
      "\n",
      "    accuracy                          0.696     34845\n",
      "   macro avg      0.692     0.694     0.693     34845\n",
      "weighted avg      0.697     0.696     0.696     34845\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.931     0.480     0.633       475\n",
      "           1      0.308     0.866     0.455       127\n",
      "\n",
      "    accuracy                          0.561       602\n",
      "   macro avg      0.619     0.673     0.544       602\n",
      "weighted avg      0.799     0.561     0.596       602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Majority vote model preds against annotator-level labels.\n",
    "cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=kumar_maj['toxic_score'],\n",
    "    y_pred=kumar_maj['majority_vote_model_predicted_toxic_score']\n",
    ")\n",
    "\n",
    "print(cm)\n",
    "print(cm.sum(axis=1) / cm.sum())\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj['toxic_score'],\n",
    "    y_pred=kumar_maj['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj[~kumar_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=kumar_maj[~kumar_maj['extreme_annotator']]['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj[kumar_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=kumar_maj[kumar_maj['extreme_annotator']]['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15184  4789]\n",
      " [ 4099 11375]]\n",
      "[0.56346094 0.43653906]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.787     0.760     0.774     19973\n",
      "           1      0.704     0.735     0.719     15474\n",
      "\n",
      "    accuracy                          0.749     35447\n",
      "   macro avg      0.746     0.748     0.746     35447\n",
      "weighted avg      0.751     0.749     0.750     35447\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.786     0.757     0.771     19498\n",
      "           1      0.705     0.738     0.721     15347\n",
      "\n",
      "    accuracy                          0.748     34845\n",
      "   macro avg      0.745     0.747     0.746     34845\n",
      "weighted avg      0.750     0.748     0.749     34845\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.856     0.888     0.872       475\n",
      "           1      0.514     0.441     0.475       127\n",
      "\n",
      "    accuracy                          0.794       602\n",
      "   macro avg      0.685     0.665     0.673       602\n",
      "weighted avg      0.784     0.794     0.788       602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SepHeads preds against annotator-level labels.\n",
    "cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=kumar_maj['toxic_score'],\n",
    "    y_pred=kumar_maj['sepheads_predicted_toxic_score']\n",
    ")\n",
    "\n",
    "print(cm)\n",
    "print(cm.sum(axis=1) / cm.sum())\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj['toxic_score'],\n",
    "    y_pred=kumar_maj['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj[~kumar_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=kumar_maj[~kumar_maj['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj[kumar_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=kumar_maj[kumar_maj['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.625     0.915     0.742     19973\n",
      "           1      0.725     0.290     0.415     15474\n",
      "\n",
      "    accuracy                          0.642     35447\n",
      "   macro avg      0.675     0.603     0.579     35447\n",
      "weighted avg      0.668     0.642     0.599     35447\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.621     0.917     0.740     19498\n",
      "           1      0.732     0.289     0.415     15347\n",
      "\n",
      "    accuracy                          0.640     34845\n",
      "   macro avg      0.677     0.603     0.578     34845\n",
      "weighted avg      0.670     0.640     0.597     34845\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.851     0.819     0.835       475\n",
      "           1      0.407     0.465     0.434       127\n",
      "\n",
      "    accuracy                          0.744       602\n",
      "   macro avg      0.629     0.642     0.634       602\n",
      "weighted avg      0.757     0.744     0.750       602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boundary model preds against annotator-level labels.\n",
    "# All annotators.\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj['toxic_score'],\n",
    "    y_pred=kumar_maj['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj[~kumar_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=kumar_maj[~kumar_maj['extreme_annotator']]['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=kumar_maj[kumar_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=kumar_maj[kumar_maj['extreme_annotator']]['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds VS MHS annotator-level labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1058  153]\n",
      " [ 215  831]]\n",
      "[0.53655295 0.46344705]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.831     0.874     0.852      1211\n",
      "           1      0.845     0.794     0.819      1046\n",
      "\n",
      "    accuracy                          0.837      2257\n",
      "   macro avg      0.838     0.834     0.835      2257\n",
      "weighted avg      0.837     0.837     0.836      2257\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.817     0.877     0.846      1028\n",
      "           1      0.857     0.790     0.822       960\n",
      "\n",
      "    accuracy                          0.835      1988\n",
      "   macro avg      0.837     0.834     0.834      1988\n",
      "weighted avg      0.837     0.835     0.835      1988\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.923     0.852     0.886       183\n",
      "           1      0.730     0.849     0.785        86\n",
      "\n",
      "    accuracy                          0.851       269\n",
      "   macro avg      0.827     0.851     0.836       269\n",
      "weighted avg      0.861     0.851     0.854       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Majority vote model preds against annotator-level labels.\n",
    "cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=mhs_maj['toxic_score'],\n",
    "    y_pred=mhs_maj['majority_vote_model_predicted_toxic_score']\n",
    ")\n",
    "\n",
    "print(cm)\n",
    "print(cm.sum(axis=1) / cm.sum())\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj['toxic_score'],\n",
    "    y_pred=mhs_maj['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj[~mhs_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=mhs_maj[~mhs_maj['extreme_annotator']]['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj[mhs_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=mhs_maj[mhs_maj['extreme_annotator']]['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1113   98]\n",
      " [ 381  665]]\n",
      "[0.53655295 0.46344705]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.745     0.919     0.823      1211\n",
      "           1      0.872     0.636     0.735      1046\n",
      "\n",
      "    accuracy                          0.788      2257\n",
      "   macro avg      0.808     0.777     0.779      2257\n",
      "weighted avg      0.804     0.788     0.782      2257\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.731     0.917     0.814      1028\n",
      "           1      0.878     0.639     0.739       960\n",
      "\n",
      "    accuracy                          0.783      1988\n",
      "   macro avg      0.805     0.778     0.777      1988\n",
      "weighted avg      0.802     0.783     0.778      1988\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.929     0.879       183\n",
      "           1      0.800     0.605     0.689        86\n",
      "\n",
      "    accuracy                          0.825       269\n",
      "   macro avg      0.817     0.767     0.784       269\n",
      "weighted avg      0.823     0.825     0.818       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SepHeads preds against annotator-level labels.\n",
    "cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=mhs_maj['toxic_score'],\n",
    "    y_pred=mhs_maj['sepheads_predicted_toxic_score']\n",
    ")\n",
    "\n",
    "print(cm)\n",
    "print(cm.sum(axis=1) / cm.sum())\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj['toxic_score'],\n",
    "    y_pred=mhs_maj['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj[~mhs_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=mhs_maj[~mhs_maj['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj[mhs_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=mhs_maj[mhs_maj['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.972     0.572     0.720      1211\n",
      "           1      0.665     0.981     0.792      1046\n",
      "\n",
      "    accuracy                          0.762      2257\n",
      "   macro avg      0.818     0.777     0.756      2257\n",
      "weighted avg      0.829     0.762     0.754      2257\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.967     0.574     0.720      1028\n",
      "           1      0.682     0.979     0.804       960\n",
      "\n",
      "    accuracy                          0.770      1988\n",
      "   macro avg      0.825     0.777     0.762      1988\n",
      "weighted avg      0.830     0.770     0.761      1988\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.563     0.720       183\n",
      "           1      0.518     1.000     0.683        86\n",
      "\n",
      "    accuracy                          0.703       269\n",
      "   macro avg      0.759     0.781     0.701       269\n",
      "weighted avg      0.846     0.703     0.708       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boundary model preds against annotator-level labels.\n",
    "# All annotators.\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj['toxic_score'],\n",
    "    y_pred=mhs_maj['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj[~mhs_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=mhs_maj[~mhs_maj['extreme_annotator']]['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=mhs_maj[mhs_maj['extreme_annotator']]['toxic_score'],\n",
    "    y_pred=mhs_maj[mhs_maj['extreme_annotator']]['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds VS HateCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_kumar_on_hatecheck = pd.read_csv(os.path.join(PREDICTIONS_DIR, 'kumar_majority_vote_predictions_on_hatecheck.csv'))\n",
    "sepheads_kumar_on_hatecheck = pd.read_csv(os.path.join(PREDICTIONS_DIR, 'kumar_sepheads_predictions_on_hatecheck.csv'))\n",
    "maj_mhs_on_hatecheck = pd.read_csv(os.path.join(PREDICTIONS_DIR, 'mhs_majority_vote_predictions_on_hatecheck.csv'))\n",
    "sepheads_mhs_on_hatecheck = pd.read_csv(os.path.join(PREDICTIONS_DIR, 'mhs_sepheads_predictions_on_hatecheck.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.684     0.279     0.396      1165\n",
      "           1      0.742     0.941     0.830      2563\n",
      "\n",
      "    accuracy                          0.734      3728\n",
      "   macro avg      0.713     0.610     0.613      3728\n",
      "weighted avg      0.724     0.734     0.694      3728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Majority vote model trained on Kumar.\n",
    "print(classification_report(\n",
    "    y_true=maj_kumar_on_hatecheck['label_gold'],\n",
    "    y_pred=maj_kumar_on_hatecheck['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.554     0.433     0.486   3358695\n",
      "           1      0.766     0.842     0.802   7389129\n",
      "\n",
      "    accuracy                          0.714  10747824\n",
      "   macro avg      0.660     0.637     0.644  10747824\n",
      "weighted avg      0.700     0.714     0.703  10747824\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.583     0.412     0.483   3178120\n",
      "           1      0.764     0.866     0.812   6991864\n",
      "\n",
      "    accuracy                          0.724  10169984\n",
      "   macro avg      0.674     0.639     0.648  10169984\n",
      "weighted avg      0.708     0.724     0.709  10169984\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.383     0.794     0.516    180575\n",
      "           1      0.817     0.418     0.553    397265\n",
      "\n",
      "    accuracy                          0.535    577840\n",
      "   macro avg      0.600     0.606     0.535    577840\n",
      "weighted avg      0.681     0.535     0.541    577840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SepHeads model trained on Kumar.\n",
    "# All annotators.\n",
    "print('All annotators')\n",
    "print(classification_report(\n",
    "    y_true=sepheads_kumar_on_hatecheck['label_gold'],\n",
    "    y_pred=sepheads_kumar_on_hatecheck['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=sepheads_kumar_on_hatecheck[~sepheads_kumar_on_hatecheck['extreme_annotator']]['label_gold'],\n",
    "    y_pred=sepheads_kumar_on_hatecheck[~sepheads_kumar_on_hatecheck['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=sepheads_kumar_on_hatecheck[sepheads_kumar_on_hatecheck['extreme_annotator']]['label_gold'],\n",
    "    y_pred=sepheads_kumar_on_hatecheck[sepheads_kumar_on_hatecheck['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.347     0.919     0.504      1165\n",
      "           1      0.854     0.214     0.342      2563\n",
      "\n",
      "    accuracy                          0.435      3728\n",
      "   macro avg      0.600     0.567     0.423      3728\n",
      "weighted avg      0.695     0.435     0.393      3728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Majority vote model trained on MHS.\n",
    "print(classification_report(\n",
    "    y_true=maj_mhs_on_hatecheck['label_gold'],\n",
    "    y_pred=maj_mhs_on_hatecheck['majority_vote_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.319     0.904     0.472    634925\n",
      "           1      0.741     0.124     0.213   1396835\n",
      "\n",
      "    accuracy                          0.368   2031760\n",
      "   macro avg      0.530     0.514     0.342   2031760\n",
      "weighted avg      0.609     0.368     0.294   2031760\n",
      "\n",
      "Non-extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.319     0.904     0.472    633760\n",
      "           1      0.741     0.124     0.213   1394272\n",
      "\n",
      "    accuracy                          0.368   2028032\n",
      "   macro avg      0.530     0.514     0.342   2028032\n",
      "weighted avg      0.609     0.368     0.294   2028032\n",
      "\n",
      "Extreme annotators\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.319     0.917     0.473      1165\n",
      "           1      0.744     0.110     0.192      2563\n",
      "\n",
      "    accuracy                          0.362      3728\n",
      "   macro avg      0.531     0.513     0.332      3728\n",
      "weighted avg      0.611     0.362     0.280      3728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SepHeads model trained on MHS.\n",
    "# All annotators.\n",
    "print('All annotators')\n",
    "print(classification_report(\n",
    "    y_true=sepheads_mhs_on_hatecheck['label_gold'],\n",
    "    y_pred=sepheads_mhs_on_hatecheck['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Non-extreme annotators.\n",
    "print('Non-extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=sepheads_mhs_on_hatecheck[~sepheads_mhs_on_hatecheck['extreme_annotator']]['label_gold'],\n",
    "    y_pred=sepheads_mhs_on_hatecheck[~sepheads_mhs_on_hatecheck['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Extreme annotators.\n",
    "print('Extreme annotators')\n",
    "print(classification_report(\n",
    "    y_true=sepheads_mhs_on_hatecheck[sepheads_mhs_on_hatecheck['extreme_annotator']]['label_gold'],\n",
    "    y_pred=sepheads_mhs_on_hatecheck[sepheads_mhs_on_hatecheck['extreme_annotator']]['sepheads_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.990     0.840     0.909      1165\n",
      "           1      0.932     0.996     0.963      2563\n",
      "\n",
      "    accuracy                          0.947      3728\n",
      "   macro avg      0.961     0.918     0.936      3728\n",
      "weighted avg      0.950     0.947     0.946      3728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boundary model.\n",
    "boundary_model_on_hatecheck = sepheads_kumar_on_hatecheck.groupby('case_id').agg(\n",
    "    boundary_model_predicted_toxic_score=pd.NamedAgg('boundary_model_predicted_toxic_score', 'first'),\n",
    "    label_gold=pd.NamedAgg('label_gold', 'first')\n",
    ")\n",
    "\n",
    "print(classification_report(\n",
    "    y_true=boundary_model_on_hatecheck['label_gold'],\n",
    "    y_pred=boundary_model_on_hatecheck['boundary_model_predicted_toxic_score'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475 127]\n",
      "[0.78903654 0.21096346]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_82f29_row0_col0 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_82f29_row0_col1 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_82f29_row1_col0 {\n",
       "  background-color: #eef5fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_82f29_row1_col1 {\n",
       "  background-color: #f5fafe;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_82f29\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_82f29_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_82f29_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_82f29_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_82f29_row0_col0\" class=\"data row0 col0\" >422</td>\n",
       "      <td id=\"T_82f29_row0_col1\" class=\"data row0 col1\" >53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_82f29_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_82f29_row1_col0\" class=\"data row1 col0\" >71</td>\n",
       "      <td id=\"T_82f29_row1_col1\" class=\"data row1 col1\" >56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe0f00bcd60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"][1].sum(axis=1))\n",
    "# get percentage\n",
    "print(confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"][1].sum(axis=1) / (confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"][1].sum()))\n",
    "pd.DataFrame(confusion_matrix[\"kumar\"][\"sepheads\"][\"kumar\"][1]).style.background_gradient(cmap='Blues', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic_score_maj</th>\n",
       "      <th>majority_vote_model_predicted_toxic_score</th>\n",
       "      <th>comment</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxic_score_sep</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>extreme_annotator</th>\n",
       "      <th>sepheads_predicted_toxic_score</th>\n",
       "      <th>boundary_model_predicted_toxic_score</th>\n",
       "      <th>boundary_model_confidence_score</th>\n",
       "      <th>toxic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>#AtoZQuiz A05 Bar [any mention of baa]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#AtoZQuiz A05 Bar [any mention of baa]</td>\n",
       "      <td>dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>#AtoZQuiz A05 Bar [any mention of baa]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#AtoZQuiz A05 Bar [any mention of baa]</td>\n",
       "      <td>29a3513367445e0fd3c53d61da1fcbebbf4efc6e0de0b9...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>#AtoZQuiz A05 Bar [any mention of baa]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#AtoZQuiz A05 Bar [any mention of baa]</td>\n",
       "      <td>26523080557217fc3b42c882aecab5863966ccfbe31c3f...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>Robert Manion????HA!More like ROBER manion bcs...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Robert Manion????HA!More like ROBER manion bcs...</td>\n",
       "      <td>dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Robert Manion????HA!More like ROBER manion bcs...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Robert Manion????HA!More like ROBER manion bcs...</td>\n",
       "      <td>29a3513367445e0fd3c53d61da1fcbebbf4efc6e0de0b9...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35442</th>\n",
       "      <td>105986</td>\n",
       "      <td>Same. Jesus Christ hes been horrible this year</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Same. Jesus Christ hes been horrible this year</td>\n",
       "      <td>8bcb34d67e6969f6e2c3f4d96db021ea3bedb73831522e...</td>\n",
       "      <td>1</td>\n",
       "      <td>3024</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35443</th>\n",
       "      <td>105986</td>\n",
       "      <td>Same. Jesus Christ hes been horrible this year</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Same. Jesus Christ hes been horrible this year</td>\n",
       "      <td>50d355ebffb4a40ef84da9137b206ac3a54b00bcc94b6c...</td>\n",
       "      <td>1</td>\n",
       "      <td>10446</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35444</th>\n",
       "      <td>105992</td>\n",
       "      <td>Last time I checked they don't measure radiati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Last time I checked they don't measure radiati...</td>\n",
       "      <td>93bb39808c33e806cf7fc28190caeca8662561dca6ca2a...</td>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35445</th>\n",
       "      <td>105992</td>\n",
       "      <td>Last time I checked they don't measure radiati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Last time I checked they don't measure radiati...</td>\n",
       "      <td>8bcb34d67e6969f6e2c3f4d96db021ea3bedb73831522e...</td>\n",
       "      <td>0</td>\n",
       "      <td>3024</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35446</th>\n",
       "      <td>105992</td>\n",
       "      <td>Last time I checked they don't measure radiati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Last time I checked they don't measure radiati...</td>\n",
       "      <td>50d355ebffb4a40ef84da9137b206ac3a54b00bcc94b6c...</td>\n",
       "      <td>0</td>\n",
       "      <td>10446</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18275 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_id                                               text   \n",
       "0            5             #AtoZQuiz A05 Bar [any mention of baa]  \\\n",
       "1            5             #AtoZQuiz A05 Bar [any mention of baa]   \n",
       "2            5             #AtoZQuiz A05 Bar [any mention of baa]   \n",
       "3           11  Robert Manion????HA!More like ROBER manion bcs...   \n",
       "4           11  Robert Manion????HA!More like ROBER manion bcs...   \n",
       "...        ...                                                ...   \n",
       "35442   105986    Same. Jesus Christ hes been horrible this year   \n",
       "35443   105986    Same. Jesus Christ hes been horrible this year   \n",
       "35444   105992  Last time I checked they don't measure radiati...   \n",
       "35445   105992  Last time I checked they don't measure radiati...   \n",
       "35446   105992  Last time I checked they don't measure radiati...   \n",
       "\n",
       "       toxic_score_maj  majority_vote_model_predicted_toxic_score   \n",
       "0                    0                                          0  \\\n",
       "1                    0                                          0   \n",
       "2                    0                                          0   \n",
       "3                    0                                          0   \n",
       "4                    0                                          0   \n",
       "...                ...                                        ...   \n",
       "35442                1                                          0   \n",
       "35443                1                                          0   \n",
       "35444                0                                          0   \n",
       "35445                0                                          0   \n",
       "35446                0                                          0   \n",
       "\n",
       "                                                 comment   \n",
       "0                 #AtoZQuiz A05 Bar [any mention of baa]  \\\n",
       "1                 #AtoZQuiz A05 Bar [any mention of baa]   \n",
       "2                 #AtoZQuiz A05 Bar [any mention of baa]   \n",
       "3      Robert Manion????HA!More like ROBER manion bcs...   \n",
       "4      Robert Manion????HA!More like ROBER manion bcs...   \n",
       "...                                                  ...   \n",
       "35442    Same. Jesus Christ hes been horrible this year   \n",
       "35443    Same. Jesus Christ hes been horrible this year   \n",
       "35444  Last time I checked they don't measure radiati...   \n",
       "35445  Last time I checked they don't measure radiati...   \n",
       "35446  Last time I checked they don't measure radiati...   \n",
       "\n",
       "                                               worker_id  toxic_score_sep   \n",
       "0      dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...                0  \\\n",
       "1      29a3513367445e0fd3c53d61da1fcbebbf4efc6e0de0b9...                0   \n",
       "2      26523080557217fc3b42c882aecab5863966ccfbe31c3f...                0   \n",
       "3      dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...                0   \n",
       "4      29a3513367445e0fd3c53d61da1fcbebbf4efc6e0de0b9...                0   \n",
       "...                                                  ...              ...   \n",
       "35442  8bcb34d67e6969f6e2c3f4d96db021ea3bedb73831522e...                1   \n",
       "35443  50d355ebffb4a40ef84da9137b206ac3a54b00bcc94b6c...                1   \n",
       "35444  93bb39808c33e806cf7fc28190caeca8662561dca6ca2a...                0   \n",
       "35445  8bcb34d67e6969f6e2c3f4d96db021ea3bedb73831522e...                0   \n",
       "35446  50d355ebffb4a40ef84da9137b206ac3a54b00bcc94b6c...                0   \n",
       "\n",
       "       annotator_id  extreme_annotator  sepheads_predicted_toxic_score   \n",
       "0                 1              False                               0  \\\n",
       "1                 2              False                               0   \n",
       "2                 3              False                               0   \n",
       "3                 1              False                               0   \n",
       "4                 2              False                               0   \n",
       "...             ...                ...                             ...   \n",
       "35442          3024              False                               1   \n",
       "35443         10446              False                               1   \n",
       "35444           640              False                               0   \n",
       "35445          3024              False                               0   \n",
       "35446         10446              False                               0   \n",
       "\n",
       "       boundary_model_predicted_toxic_score  boundary_model_confidence_score   \n",
       "0                                         0                               90  \\\n",
       "1                                         0                               90   \n",
       "2                                         0                               90   \n",
       "3                                         0                               90   \n",
       "4                                         0                               90   \n",
       "...                                     ...                              ...   \n",
       "35442                                     0                               90   \n",
       "35443                                     0                               90   \n",
       "35444                                     0                               95   \n",
       "35445                                     0                               95   \n",
       "35446                                     0                               95   \n",
       "\n",
       "       toxic_score  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "35442            1  \n",
       "35443            1  \n",
       "35444            0  \n",
       "35445            0  \n",
       "35446            0  \n",
       "\n",
       "[18275 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = results_df[\"kumar\"][\"majority\"][\"kumar\"]\n",
    "b = results_df[\"kumar\"][\"sepheads\"][\"kumar\"]\n",
    "# check if toxic score is the same \n",
    "a[a[\"toxic_score\"] == b[\"toxic_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check: re-compute all metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained models' metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: kumar | Eval dataset: kumar | Model: majority_vote\n",
      "0.69 0.693 0.544 0.678 0.676 0.866\n",
      "\n",
      "\n",
      "Training dataset: kumar | Eval dataset: kumar | Model: sepheads\n",
      "0.746 0.746 0.673 0.735 0.738 0.441\n",
      "\n",
      "\n",
      "Training dataset: - | Eval dataset: kumar | Model: boundary\n",
      "0.579 0.578 0.634 0.29 0.289 0.465\n",
      "\n",
      "\n",
      "Training dataset: kumar | Eval dataset: hatecheck | Model: majority_vote\n",
      "0.613 0.941\n",
      "\n",
      "\n",
      "Training dataset: kumar | Eval dataset: hatecheck | Model: sepheads\n",
      "0.644 0.648 0.535 0.842 0.866 0.418\n",
      "\n",
      "\n",
      "Training dataset: - | Eval dataset: hatecheck | Model: boundary\n",
      "0.936 0.996\n",
      "\n",
      "\n",
      "Training dataset: mhs | Eval dataset: mhs | Model: majority_vote\n",
      "0.835 0.834 0.836 0.794 0.79 0.849\n",
      "\n",
      "\n",
      "Training dataset: mhs | Eval dataset: mhs | Model: sepheads\n",
      "0.779 0.777 0.784 0.636 0.639 0.605\n",
      "\n",
      "\n",
      "Training dataset: - | Eval dataset: mhs | Model: boundary\n",
      "0.756 0.762 0.701 0.981 0.979 1.0\n",
      "\n",
      "\n",
      "Training dataset: mhs | Eval dataset: hatecheck | Model: majority_vote\n",
      "0.423 0.214\n",
      "\n",
      "\n",
      "Training dataset: mhs | Eval dataset: hatecheck | Model: sepheads\n",
      "0.342 0.342 0.332 0.124 0.124 0.11\n",
      "\n",
      "\n",
      "Training dataset: - | Eval dataset: hatecheck | Model: boundary\n",
      "0.936 0.996\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_reports = {}\n",
    "\n",
    "for training_dataset in ['kumar', 'mhs']:\n",
    "    classification_reports[training_dataset] = {}\n",
    "    \n",
    "    multi_annotator_dataset = 'kumar' if (training_dataset == 'kumar') else 'mhs'\n",
    "    \n",
    "    for eval_dataset in [multi_annotator_dataset, 'hatecheck']:\n",
    "        classification_reports[training_dataset][eval_dataset] = {}\n",
    "        \n",
    "        for model in ['majority_vote', 'sepheads']:\n",
    "            print(f'Training dataset: {training_dataset} | Eval dataset: {eval_dataset} | Model: {model}')\n",
    "\n",
    "            classification_reports[training_dataset][eval_dataset][model] = {}\n",
    "\n",
    "            # Load predictions.\n",
    "            pred_df = pd.read_csv(os.path.join(PREDICTIONS_DIR, f'{training_dataset}_{model}_predictions_on_{eval_dataset}.csv'))\n",
    "\n",
    "            # If the training dataset is a multi-annotator one and the model\n",
    "            # is a majority-vote one, boradcast (join) the model's predictions\n",
    "            # across all annotators.\n",
    "            if (eval_dataset != 'hatecheck') and (model == 'majority_vote'):\n",
    "                # Load the corresponding predictions from SepHeads (just to have all the annotators).\n",
    "                annotator_level_data = pd.read_csv(os.path.join(PREDICTIONS_DIR, f'{training_dataset}_sepheads_predictions_on_{eval_dataset}.csv'))[\n",
    "                    ['text_id', 'annotator_id', 'extreme_annotator', 'toxic_score']\n",
    "                ]\n",
    "\n",
    "                # Join with the predictions from the majority vote model.\n",
    "                pred_df = pd.merge(\n",
    "                    left=annotator_level_data,\n",
    "                    right=pred_df[['text_id', 'majority_vote_model_predicted_toxic_score']],\n",
    "                    how='left',\n",
    "                    on='text_id'\n",
    "                )\n",
    "\n",
    "            pred_column = 'majority_vote_model_predicted_toxic_score' if model == 'majority_vote' else 'sepheads_predicted_toxic_score'\n",
    "            ground_truth_column = 'toxic_score' if eval_dataset != 'hatecheck' else 'label_gold'\n",
    "\n",
    "            cr_all = classification_report(\n",
    "                y_true=pred_df[ground_truth_column],\n",
    "                y_pred=pred_df[pred_column],\n",
    "                digits=3,\n",
    "                output_dict=True\n",
    "            )\n",
    "\n",
    "            classification_reports[training_dataset][eval_dataset][model]['all'] = cr_all\n",
    "\n",
    "            if (eval_dataset != 'hatecheck') or (model != 'majority_vote'):\n",
    "                cr_nonextreme = classification_report(\n",
    "                    y_true=pred_df[~pred_df['extreme_annotator']][ground_truth_column],\n",
    "                    y_pred=pred_df[~pred_df['extreme_annotator']][pred_column],\n",
    "                    digits=3,\n",
    "                    output_dict=True\n",
    "                )\n",
    "\n",
    "                classification_reports[training_dataset][eval_dataset][model]['nonextreme'] = cr_nonextreme\n",
    "\n",
    "                cr_extreme = classification_report(\n",
    "                    y_true=pred_df[pred_df['extreme_annotator']][ground_truth_column],\n",
    "                    y_pred=pred_df[pred_df['extreme_annotator']][pred_column],\n",
    "                    digits=3,\n",
    "                    output_dict=True\n",
    "                )\n",
    "\n",
    "                classification_reports[training_dataset][eval_dataset][model]['extreme'] = cr_extreme\n",
    "\n",
    "                print(\n",
    "                    round(cr_all['macro avg']['f1-score'], 3),\n",
    "                    round(cr_nonextreme['macro avg']['f1-score'], 3),\n",
    "                    round(cr_extreme['macro avg']['f1-score'], 3),\n",
    "                    round(cr_all['1']['recall'], 3),\n",
    "                    round(cr_nonextreme['1']['recall'], 3),\n",
    "                    round(cr_extreme['1']['recall'], 3)\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    round(cr_all['macro avg']['f1-score'], 3),\n",
    "                    round(cr_all['1']['recall'], 3),\n",
    "                )\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "            if model == 'sepheads':\n",
    "                if eval_dataset == 'hatecheck':\n",
    "                    print(f'Training dataset: - | Eval dataset: {eval_dataset} | Model: boundary')\n",
    "                    \n",
    "                    boundary_model_pred_df = pred_df.groupby('case_id').agg(\n",
    "                        boundary_model_predicted_toxic_score=pd.NamedAgg('boundary_model_predicted_toxic_score', 'first'),\n",
    "                        label_gold=pd.NamedAgg('label_gold', 'first')\n",
    "                    ).reset_index()\n",
    "    \n",
    "                    cr_all = classification_report(\n",
    "                        y_true=boundary_model_pred_df['label_gold'],\n",
    "                        y_pred=boundary_model_pred_df['boundary_model_predicted_toxic_score'],\n",
    "                        digits=3,\n",
    "                        output_dict=True\n",
    "                    )\n",
    "    \n",
    "                    print(\n",
    "                        round(cr_all['macro avg']['f1-score'], 3),\n",
    "                        round(cr_all['1']['recall'], 3),\n",
    "                    )\n",
    "    \n",
    "                    print('\\n')\n",
    "                else:\n",
    "                    print(f'Training dataset: - | Eval dataset: {eval_dataset} | Model: boundary')\n",
    "\n",
    "                    cr_all = classification_report(\n",
    "                        y_true=pred_df[ground_truth_column],\n",
    "                        y_pred=pred_df['boundary_model_predicted_toxic_score'],\n",
    "                        digits=3,\n",
    "                        output_dict=True\n",
    "                    )\n",
    "\n",
    "                    cr_nonextreme = classification_report(\n",
    "                        y_true=pred_df[~pred_df['extreme_annotator']][ground_truth_column],\n",
    "                        y_pred=pred_df[~pred_df['extreme_annotator']]['boundary_model_predicted_toxic_score'],\n",
    "                        digits=3,\n",
    "                        output_dict=True\n",
    "                    )\n",
    "    \n",
    "                    cr_extreme = classification_report(\n",
    "                        y_true=pred_df[pred_df['extreme_annotator']][ground_truth_column],\n",
    "                        y_pred=pred_df[pred_df['extreme_annotator']]['boundary_model_predicted_toxic_score'],\n",
    "                        digits=3,\n",
    "                        output_dict=True\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        round(cr_all['macro avg']['f1-score'], 3),\n",
    "                        round(cr_nonextreme['macro avg']['f1-score'], 3),\n",
    "                        round(cr_extreme['macro avg']['f1-score'], 3),\n",
    "                        round(cr_all['1']['recall'], 3),\n",
    "                        round(cr_nonextreme['1']['recall'], 3),\n",
    "                        round(cr_extreme['1']['recall'], 3)\n",
    "                    )\n",
    "\n",
    "                    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy model's metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset: kumar | Model: random\n",
      "0.493 0.494 0.438 0.495 0.495 0.425\n",
      "Eval dataset: mhs | Model: random\n",
      "0.489 0.491 0.468 0.487 0.486 0.488\n",
      "Eval dataset: hatecheck | Model: random\n",
      "0.482 0.482 0.491 0.5 0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "for eval_dataset in ['kumar', 'mhs', 'hatecheck']:\n",
    "    print(f'Eval dataset: {eval_dataset} | Model: random')\n",
    "    \n",
    "    # Load predictions from the sepheads model (the Kumar one, for hatecheck)\n",
    "    # to get the samples.\n",
    "    dummy_training_dataset = {\n",
    "        'kumar': 'kumar',\n",
    "        'mhs': 'mhs',\n",
    "        'hatecheck': 'mhs'\n",
    "    }\n",
    "\n",
    "    pred_df = pd.read_csv(os.path.join(PREDICTIONS_DIR, f'{dummy_training_dataset[eval_dataset]}_sepheads_predictions_on_{eval_dataset}.csv'))\n",
    "    \n",
    "    ground_truth_column = 'toxic_score' if eval_dataset != 'hatecheck' else 'label_gold'\n",
    "\n",
    "    random_model = DummyClassifier(strategy='uniform')\n",
    "    random_model.fit(pred_df[ground_truth_column], pred_df[ground_truth_column])\n",
    "    \n",
    "    pred_df['random_model_predicted_toxic_score'] =  random_model.predict(pred_df[ground_truth_column])\n",
    "\n",
    "    cr_all = classification_report(\n",
    "        y_true=pred_df[ground_truth_column],\n",
    "        y_pred=pred_df['random_model_predicted_toxic_score'],\n",
    "        digits=3,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    cr_nonextreme = classification_report(\n",
    "        y_true=pred_df[~pred_df['extreme_annotator']][ground_truth_column],\n",
    "        y_pred=pred_df[~pred_df['extreme_annotator']]['random_model_predicted_toxic_score'],\n",
    "        digits=3,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    cr_extreme = classification_report(\n",
    "        y_true=pred_df[pred_df['extreme_annotator']][ground_truth_column],\n",
    "        y_pred=pred_df[pred_df['extreme_annotator']]['random_model_predicted_toxic_score'],\n",
    "        digits=3,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        round(cr_all['macro avg']['f1-score'], 3),\n",
    "        round(cr_nonextreme['macro avg']['f1-score'], 3),\n",
    "        round(cr_extreme['macro avg']['f1-score'], 3),\n",
    "        round(cr_all['1']['recall'], 3),\n",
    "        round(cr_nonextreme['1']['recall'], 3),\n",
    "        round(cr_extreme['1']['recall'], 3)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
