{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e668dea0-ea3a-4781-85ca-4b13ba43d703",
   "metadata": {},
   "source": [
    "# Annotator modeling experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b6499-9201-4dbe-81cb-328fb7e55b26",
   "metadata": {},
   "source": [
    "__Objective:__ develop a model for toxicity detection that models annotators as well as data, so that possibly diverging opinions can be consistently captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831edaa-7ec1-410a-b8c0-8a2398d7823b",
   "metadata": {},
   "source": [
    "Main sources:<br>\n",
    "[1] [\"Architectural sweet spots\" paper](https://aclanthology.org/2023.emnlp-main.687/)<br>\n",
    "[2] [\"Jury Learning\" paper](https://arxiv.org/abs/2202.02950)\n",
    "\n",
    "[1] is the main source. Here, we'll try to implement one of the architectures they suggested.\n",
    "\n",
    "Other sources (fine-tuning scripts, docs, ...):\n",
    "- [Script for fine-tuning RoBERTa models](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py)\n",
    "- [Hugging Face RoBERTa docs](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/roberta)\n",
    "- [Hugging Face source code for RoBERTa models](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bce77-8e7b-404b-a4e9-74ed2b93d514",
   "metadata": {},
   "source": [
    "Modelling plan (from lower to higher complexity, i.e. from less to more annotator modelling, see the [meeting notes](https://docs.google.com/document/d/1YRPDi0JVk2ijyNm_TURZWQW_wUkves-2S_3j3ZWhw-U/edit?pli=1&tab=t.0#heading=h.bt8gf8u1l9h4)):\n",
    "1. Labels aggregated over annotators by majority vote, no annotator modelling at all (baseline).\n",
    "2. *SepHeads* architecture (from [1]): a single LLM for text encoding (RoBERTa), with classification heads fine-tuned for each annotator.\n",
    "3. *ShareREC* architecture (again from [1]): a single LLM for text encoding (RoBERTa, as above), a separate network for annotator encoding and a \"neural combiner\" component (either a feed-forward NN or a Deep Cross Network (DCN)) for merging the two pieces of information together and producing an output classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e655a7-ee81-4fe5-9345-395dd5ce1e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71da540-cd86-421c-9ab7-2302d575174b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12108d81-04e4-479b-939b-a2ea5759d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'popquorn': '../data/samples/POPQUORN_offensiveness.csv',\n",
    "    'kumar': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv',\n",
    "        # 'train':  '/data/milanlp/moscato/personal_hate_bounds_data/kumar_processed_with_ID_and_full_perspective_clean.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv',\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASET_NAME = 'kumar'\n",
    "\n",
    "training_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['train'])\n",
    "test_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a7fb6c-f816-4d7c-a490-a1857aa44aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17110,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N annotators.\n",
    "training_data['worker_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e43e9dd-fc9b-4cb1-974d-2a1a7a9b9e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min N annotations per annotator.\n",
    "training_data.groupby('worker_id')['text_id'].count().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce28855-177e-45be-8f55-7b68e773b900",
   "metadata": {},
   "source": [
    "## DeBERTa model for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df02c2a-8606-4b0c-9876-5ded56bd8825",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- When instantiating a `RobertaForSequenceClassification` model, the classification head is instantiated automatically inside it, reading the `num_labels` and `classifier_dropout` parameters from the config. Unfortunately, the hidden size of the classification head is read from the `hidden_size` parameter of the config, which is also read by the encoder, so the two must be equal (and equal to 768 for pretrained model). Alternative: define a RoBERTa encoder with the default classifier head, then substitute it with a new one instantiated using a separate config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690de184-a0ac-493b-b15c-41092918a015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model_utils import get_deberta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af59af43-f41c-408d-b6c8-46c8a250fa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:01:28,176 - get_deberta_model - INFO - Instantiating DeBERTa tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N labels found in training data: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:01:29,271 - get_deberta_model - INFO - Instantiating DeBERTa model with default classification head\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = training_data['toxic_score'].unique().shape[0]\n",
    "\n",
    "model_dir = '/data1/shared_models/'\n",
    "\n",
    "print('N labels found in training data:', num_labels)\n",
    "\n",
    "deberta_tokenizer, deberta_model = get_deberta_model(\n",
    "    num_labels,\n",
    "    model_dir,\n",
    "    device,\n",
    "    use_custom_head=False,\n",
    "    pooler_out_features=768,  # Default: 768.\n",
    "    pooler_drop_prob=0.0,  # Default: 0.0\n",
    "    classifier_drop_prob=0.1,  # Default: 0.1\n",
    "    use_fast_tokenizer=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e75215a-21a9-4281-b8d6-bf6e18c9504a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old experiment with a RoBERTa model.\n",
    "# # from transformers import AutoConfig, PretrainedConfig, AutoTokenizer, RobertaForSequenceClassification, pipeline\n",
    "# # from transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n",
    "\n",
    "# model_id = 'roberta-base'\n",
    "\n",
    "# # Config for the encoder.\n",
    "# roberta_classifier_config = AutoConfig.from_pretrained(\n",
    "#     model_id,\n",
    "#     finetuning_task=\"text-classification\",\n",
    "#     id2label={\n",
    "#         0: 'non-toxic',\n",
    "#         1: 'toxic'\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Config for the classification head. These are all the\n",
    "# # parameters a `RobertaClassificationHead` requires.\n",
    "# roberta_classification_head_config = PretrainedConfig()\n",
    "\n",
    "# roberta_classification_head_config.classifier_dropout = 0.1\n",
    "# roberta_classification_head_config.hidden_size = 768\n",
    "# roberta_classification_head_config.num_labels = 2\n",
    "\n",
    "\n",
    "# # Instantiate tokenizer.\n",
    "# roberta_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# # Instantiate RoBERTa model.\n",
    "# roberta_classifier = RobertaForSequenceClassification.from_pretrained(\n",
    "#     'roberta-base',\n",
    "#     config=roberta_classifier_config,\n",
    "# )\n",
    "\n",
    "# # Substitute the default classification head with a custom one.\n",
    "# roberta_classifier.classifier = RobertaClassificationHead(roberta_classification_head_config)\n",
    "\n",
    "\n",
    "# # Put everything together in a single pipeline object.\n",
    "# roberta_classifier_pipeline = pipeline(\n",
    "#     task='text-classification',\n",
    "#     config=roberta_classifier_config,\n",
    "#     tokenizer=roberta_tokenizer,\n",
    "#     model=roberta_classifier\n",
    "# )\n",
    "\n",
    "# roberta_classifier_pipeline(data_df.iloc[:12]['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950e513-2c7f-4888-ab7f-eee0bb3f0f0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## RoBERTa model for text encoding + classification head (old experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b38053-a730-447e-876b-d15a6a886d9d",
   "metadata": {},
   "source": [
    "Text encoding:\n",
    "- RoBERTa outputs two tensors:\n",
    "    - Latent representation of the `<cls>` token (`model(**encoded_input).last_hidden_state[:, 0, :]`, where the first dimension is the batch size).\n",
    "    - Output of the former, passed through a \"RoBERTa pooler\" linear layer with tanh activation (`model(**encoded_input).pooler_output`).\n",
    "- From [this issue](https://github.com/huggingface/transformers/issues/8776) it looks like the representation fed into the classification head is actually the pooled one, but the classification head only works with the full output of the encoder as its input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5edc3086-9b45-475b-a82f-262850b546ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caeea25-e8e8-4915-a0d9-d1a9b7e756e3",
   "metadata": {},
   "source": [
    "Tokenization and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2729720d-2692-4b6c-bb65-ac620146af4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "text_encoder = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "roberta_config = RobertaConfig.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4821abc5-f7a4-4fad-9c8e-66fe8ee92968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(data_df.iloc[:2]['text'].tolist(), return_tensors='pt', padding=True)\n",
    "\n",
    "output = text_encoder(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e568bd8-9584-47b0-a45c-54871928c9f1",
   "metadata": {},
   "source": [
    "Classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ff2c42-7308-4511-9d06-f4fbdfb45fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate config for the classification head.\n",
    "roberta_classification_head_config = PretrainedConfig()\n",
    "\n",
    "roberta_classification_head_config.classifier_dropout = 0.1\n",
    "roberta_classification_head_config.hidden_size = 768\n",
    "roberta_classification_head_config.num_labels = 5\n",
    "\n",
    "# Instantiate the classification head.\n",
    "classification_head = RobertaClassificationHead(roberta_classification_head_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d1ed3f-5d46-4c19-81b8-36855894ed30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClassificationHead(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae043985-7bc9-4c2f-9613-e7a8a1171119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100,  0.2229,  0.2679, -0.0165,  0.1054],\n",
       "        [-0.0244,  0.2138,  0.2665, -0.0081,  0.1243]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = classification_head(output.last_hidden_state)\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61286fcd-f373-4c48-8326-60da3e8378d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_roberta.RobertaModel"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156fbd9-d8fb-4b55-8fcb-28a6cec2ed5d",
   "metadata": {},
   "source": [
    "## DeBERTa model with annotator-specific classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad25f6-3ffe-45f8-a05b-4933d7b30f34",
   "metadata": {},
   "source": [
    "__Objective:__ create a model that\n",
    "- uses the body of a pre-trained DeBERTa model for text encoding,\n",
    "- has a different classification head for each annotator and according to the selected annotator uses that head to make the prediction.\n",
    "\n",
    "**Notes:**\n",
    "- In some form, the annotator's ID **must** be included among the model's inputs.\n",
    "- The annotator's ID **cannot** be included directly as the input, indices must be used. Reason: HF `Dataset`s object return batches of samples as lists, which are then converted to PyTorch tensors by a `DataCollator` object, but the conversion doesn't work for a list of strings. Therefore, **an explicit mapping between the `worker_id` field in the dataset (string) and integer IDs must be created**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a7632-be10-42c3-bb19-b32287082be6",
   "metadata": {},
   "source": [
    "Explore the architecture of the DeBERTa model (`DebertaV2ForSequenceClassification` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea189bb-2665-4be3-a4f2-570348ef08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_tensor_dict_to_device(tensor_dict, device):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return {\n",
    "        k: v.to(device=device)\n",
    "        for k, v in tensor_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b3e053-d21d-4d1d-a5a0-28a5231673ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 54, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_samples = training_data['comment'].sample(7).tolist()\n",
    "random_samples_tokenized = send_tensor_dict_to_device(\n",
    "    deberta_tokenizer(\n",
    "        random_samples,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ),\n",
    "    device\n",
    ")\n",
    "\n",
    "# Get the latent representation for each token in each sequence\n",
    "# in the batch via the DeBERTa encoder.\n",
    "with torch.no_grad():\n",
    "    tokens_latent_reps = deberta_model.deberta(**random_samples_tokenized)['last_hidden_state']\n",
    "\n",
    "# Shape: (batch_size, seq_len, hidden_dim).\n",
    "tokens_latent_reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe3912a-b0a2-4839-ac34-35a5a76d1781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Note: latent representations are put in a `BaseModelOutput`\n",
    "#       object implemented as an OrderedDict with indexing\n",
    "#       as well.\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        (deberta_model.deberta(**random_samples_tokenized)['last_hidden_state']\n",
    "         == deberta_model.deberta(**random_samples_tokenized)[0]).all()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3821322-e680-4293-b650-0d5793889c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0290, -0.1493],\n",
       "        [-0.0317, -0.1563],\n",
       "        [-0.0439, -0.1568],\n",
       "        [-0.0301, -0.1575],\n",
       "        [-0.0301, -0.1561],\n",
       "        [-0.0308, -0.1580],\n",
       "        [-0.0412, -0.1559]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get logits from the DeBERTa model by applying\n",
    "# sequentially: encoder -> pooler -> dropout -> classifier.\n",
    "with torch.no_grad():\n",
    "    test_logits = deberta_model.classifier(\n",
    "        deberta_model.dropout(\n",
    "            deberta_model.pooler(\n",
    "                deberta_model.deberta(**random_samples_tokenized)[0]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Shape: (batch_size, num_labels).\n",
    "test_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cec41e-1818-43c0-8d0d-7e77f3800081",
   "metadata": {},
   "source": [
    "Test the DeBERTa model with annotator-specific heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14904557-46f1-4a9f-a778-7e5c3b9c2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DebertaWithAnnotatorHeads\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ab9f7cd-a49b-4adf-acb3-7485283fa079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotator_ids = [0, 1]\n",
    "\n",
    "deberta_with_annotator_heads_model = DebertaWithAnnotatorHeads(\n",
    "    deberta_encoder=deepcopy(deberta_model.deberta),\n",
    "    deberta_pooler=deepcopy(deberta_model.pooler),\n",
    "    deberta_dropout=deepcopy(deberta_model.dropout),\n",
    "    num_labels=num_labels,\n",
    "    annotator_ids=test_annotator_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "907b4243-e13d-4fb1-9ca6-8cfd04e096fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>text_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxic_score</th>\n",
       "      <th>extreme_annotator</th>\n",
       "      <th>annotator_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just a matter of time before pick up on this s...</td>\n",
       "      <td>0</td>\n",
       "      <td>24482c451b411b96d2c2880bafbab9884007e000d143c0...</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>this is QUINN you DUMBASS 😭😭😭</td>\n",
       "      <td>1</td>\n",
       "      <td>24482c451b411b96d2c2880bafbab9884007e000d143c0...</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just a matter of time before pick up on this s...</td>\n",
       "      <td>0</td>\n",
       "      <td>dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>this is QUINN you DUMBASS 😭😭😭</td>\n",
       "      <td>1</td>\n",
       "      <td>dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  text_id  \\\n",
       "0  Just a matter of time before pick up on this s...        0   \n",
       "5                      this is QUINN you DUMBASS 😭😭😭        1   \n",
       "1  Just a matter of time before pick up on this s...        0   \n",
       "6                      this is QUINN you DUMBASS 😭😭😭        1   \n",
       "\n",
       "                                           worker_id  toxic_score  \\\n",
       "0  24482c451b411b96d2c2880bafbab9884007e000d143c0...            0   \n",
       "5  24482c451b411b96d2c2880bafbab9884007e000d143c0...            0   \n",
       "1  dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...            0   \n",
       "6  dbc501198ada6725d8e8cc6f0101824f04d4b4b8935059...            1   \n",
       "\n",
       "  extreme_annotator  annotator_id  \n",
       "0                no             0  \n",
       "5                no             0  \n",
       "1                no             1  \n",
       "6                no             1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = pd.concat([\n",
    "    training_data[training_data['annotator_id'] == annotator_id].iloc[:2]\n",
    "    for annotator_id in test_annotator_ids\n",
    "])\n",
    "test_samples_tokenized = send_tensor_dict_to_device(\n",
    "    deberta_tokenizer(test_samples['comment'].tolist(), padding=True, return_tensors='pt'),\n",
    "    device\n",
    ")\n",
    "\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f091f0d-ede4-439d-adc3-6a17ab7f3faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-0.0964, -0.0032],\n",
       "         [-0.0991,  0.0088],\n",
       "         [-0.1300,  0.0346],\n",
       "         [-0.1335,  0.0504]], device='cuda:0')}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta_with_annotator_heads_model.eval()\n",
    "# deberta_with_annotator_heads_model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_logits = deberta_with_annotator_heads_model(\n",
    "        **test_samples_tokenized,\n",
    "        annotator_ids=test_samples['annotator_id'].tolist()\n",
    "    )\n",
    "\n",
    "# Shape: (batch_size, num_labels).\n",
    "test_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fc8ee67-7caa-47cb-afc7-ad95b72519ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.7184, device='cuda:0'),\n",
       " 'logits': tensor([[-0.0964, -0.0032],\n",
       "         [-0.0991,  0.0088],\n",
       "         [-0.1300,  0.0346],\n",
       "         [-0.1335,  0.0504]], device='cuda:0')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass including the labels among the inputs.\n",
    "with torch.no_grad():\n",
    "    test_logits = deberta_with_annotator_heads_model(\n",
    "        **test_samples_tokenized,\n",
    "        annotator_ids=test_samples['annotator_id'].tolist(),\n",
    "        labels=torch.tensor(test_samples['toxic_score'].values).to(device=device)\n",
    "    )\n",
    "\n",
    "# Shape: (batch_size, num_labels).\n",
    "test_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaad3e-347a-4a27-90d1-d14f625ec91a",
   "metadata": {},
   "source": [
    "Model training.\n",
    "\n",
    "Source for building custom models compatible with the Hugging Face Transformers framework:\n",
    "- [Hugging Face custom models](https://huggingface.co/docs/transformers/custom_models)\n",
    "- [Related discussion](https://discuss.huggingface.co/t/using-huggingface-trainer-for-custom-models/16882/6)\n",
    "- [Resources](https://discuss.huggingface.co/t/resources-for-using-custom-models-with-trainer/4151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9fcaa19-2aa0-4139-aa9c-100fe860669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "from training_metrics import compute_metrics_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75041e4a-4f3e-41a4-b5f8-3083ec70c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return deberta_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        # return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54dcc935-b525-4b04-9988-ed0a757c37c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 703.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'annotator_ids', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing.\n",
    "test_ds = datasets.Dataset.from_dict(\n",
    "    test_samples[[\n",
    "        'comment',\n",
    "        'toxic_score',\n",
    "        'annotator_id'\n",
    "    ]].rename(\n",
    "        columns={\n",
    "            'comment': 'text',\n",
    "            'toxic_score': 'label',\n",
    "            'annotator_id': 'annotator_ids',\n",
    "        }\n",
    "    )\n",
    "    .to_dict(orient='list')\n",
    ")\n",
    "\n",
    "tokenized_test_ds = (\n",
    "    test_ds\n",
    "    .map(tokenize_function, batched=True)\n",
    "    .remove_columns(\"text\")\n",
    ")\n",
    "\n",
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e0ab157-c324-4b16-9588-dba50c796f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_415705/1919464298.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.674811</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.660247</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.653036</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>0.645996</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.639590</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>0.634191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.630148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.627633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_ID = 'sepheads_model_test'\n",
    "MODEL_OUTPUT_DIR = f'/data1/moscato/personalised-hate-boundaries-data/models/{EXPERIMENT_ID}/'\n",
    "N_EPOCHS = 10\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  # Options: 'no', 'epoch', 'steps' (requires the `save_steps` argument to be set though).\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,  # Default: 8.\n",
    "    gradient_accumulation_steps=1,  # Default: 1.\n",
    "    per_device_eval_batch_size=4,  # Default: 8.\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.0,  # For linear warmup of learning rate.\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    # label_names=list(roberta_classifier.config.id2label.keys()),\n",
    "    logging_strategy='epoch',\n",
    "    logging_first_step=True,\n",
    "    logging_dir=None,\n",
    "    # logging_steps=10,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=deberta_tokenizer)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=deberta_with_annotator_heads_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test_ds,\n",
    "    eval_dataset=tokenized_test_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=deberta_tokenizer,\n",
    "    compute_metrics=compute_metrics_sklearn,\n",
    ")\n",
    "\n",
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fe2fe-a306-4e01-b630-acf311e7fcdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annotator encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68306abb-4f33-4d45-b90d-f767efe104b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31a121-2739-4e69-8390-57d5047cff74",
   "metadata": {},
   "source": [
    "One-hot encoding of the annotators' categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f621009-4b91-4f24-8f90-e3bf0ddd1233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotators_categorical_features = [\n",
    "    'gender',\n",
    "    'race',\n",
    "    'age',\n",
    "    'occupation',\n",
    "    'education'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7869d07-94d7-4a60-b4c8-01acae97fd52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>offensiveness</th>\n",
       "      <th>gender_Man</th>\n",
       "      <th>gender_Non-binary</th>\n",
       "      <th>gender_Woman</th>\n",
       "      <th>race_Arab American</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black or African American</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation_Prefer not to disclose</th>\n",
       "      <th>occupation_Retired</th>\n",
       "      <th>occupation_Self-employed</th>\n",
       "      <th>occupation_Student</th>\n",
       "      <th>occupation_Unemployed</th>\n",
       "      <th>education_College degree</th>\n",
       "      <th>education_Graduate degree</th>\n",
       "      <th>education_High school diploma or equivalent</th>\n",
       "      <th>education_Less than a high school diploma</th>\n",
       "      <th>education_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530</td>\n",
       "      <td>0</td>\n",
       "      <td>I think a lot of Dethklok songs use drop C, wo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>There are relatively simple ways around all of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>Tell the british soldier in WW1 to shoot that ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "      <td>Top comment pretty much. I have gay friends an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>635</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't tell them just let them and their liniag...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13031</th>\n",
       "      <td>471</td>\n",
       "      <td>262</td>\n",
       "      <td>They’re closed anti-vaxx Facebook groups. When...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13032</th>\n",
       "      <td>1033</td>\n",
       "      <td>262</td>\n",
       "      <td>Bioethics; an interesting field in which the w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13033</th>\n",
       "      <td>740</td>\n",
       "      <td>262</td>\n",
       "      <td>Or they are really secure but hang around inse...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13034</th>\n",
       "      <td>894</td>\n",
       "      <td>262</td>\n",
       "      <td>Don't have to worry about being too big to fit...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13035</th>\n",
       "      <td>596</td>\n",
       "      <td>262</td>\n",
       "      <td>Totally agree with this sentiment. I’m fully i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13036 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instance_id  user_id  \\\n",
       "0              530        0   \n",
       "1             1280        0   \n",
       "2              621        0   \n",
       "3              676        0   \n",
       "4              635        0   \n",
       "...            ...      ...   \n",
       "13031          471      262   \n",
       "13032         1033      262   \n",
       "13033          740      262   \n",
       "13034          894      262   \n",
       "13035          596      262   \n",
       "\n",
       "                                                    text  offensiveness  \\\n",
       "0      I think a lot of Dethklok songs use drop C, wo...            1.0   \n",
       "1      There are relatively simple ways around all of...            1.0   \n",
       "2      Tell the british soldier in WW1 to shoot that ...            1.0   \n",
       "3      Top comment pretty much. I have gay friends an...            1.0   \n",
       "4      Don't tell them just let them and their liniag...            3.0   \n",
       "...                                                  ...            ...   \n",
       "13031  They’re closed anti-vaxx Facebook groups. When...            1.0   \n",
       "13032  Bioethics; an interesting field in which the w...            1.0   \n",
       "13033  Or they are really secure but hang around inse...            5.0   \n",
       "13034  Don't have to worry about being too big to fit...            4.0   \n",
       "13035  Totally agree with this sentiment. I’m fully i...            1.0   \n",
       "\n",
       "       gender_Man  gender_Non-binary  gender_Woman  race_Arab American  \\\n",
       "0             1.0                0.0           0.0                 0.0   \n",
       "1             1.0                0.0           0.0                 0.0   \n",
       "2             1.0                0.0           0.0                 0.0   \n",
       "3             1.0                0.0           0.0                 0.0   \n",
       "4             1.0                0.0           0.0                 0.0   \n",
       "...           ...                ...           ...                 ...   \n",
       "13031         0.0                0.0           1.0                 0.0   \n",
       "13032         0.0                0.0           1.0                 0.0   \n",
       "13033         0.0                0.0           1.0                 0.0   \n",
       "13034         0.0                0.0           1.0                 0.0   \n",
       "13035         0.0                0.0           1.0                 0.0   \n",
       "\n",
       "       race_Asian  race_Black or African American  ...  \\\n",
       "0             0.0                             0.0  ...   \n",
       "1             0.0                             0.0  ...   \n",
       "2             0.0                             0.0  ...   \n",
       "3             0.0                             0.0  ...   \n",
       "4             0.0                             0.0  ...   \n",
       "...           ...                             ...  ...   \n",
       "13031         1.0                             0.0  ...   \n",
       "13032         1.0                             0.0  ...   \n",
       "13033         1.0                             0.0  ...   \n",
       "13034         1.0                             0.0  ...   \n",
       "13035         1.0                             0.0  ...   \n",
       "\n",
       "       occupation_Prefer not to disclose  occupation_Retired  \\\n",
       "0                                    0.0                 0.0   \n",
       "1                                    0.0                 0.0   \n",
       "2                                    0.0                 0.0   \n",
       "3                                    0.0                 0.0   \n",
       "4                                    0.0                 0.0   \n",
       "...                                  ...                 ...   \n",
       "13031                                0.0                 0.0   \n",
       "13032                                0.0                 0.0   \n",
       "13033                                0.0                 0.0   \n",
       "13034                                0.0                 0.0   \n",
       "13035                                0.0                 0.0   \n",
       "\n",
       "       occupation_Self-employed  occupation_Student  occupation_Unemployed  \\\n",
       "0                           0.0                 0.0                    1.0   \n",
       "1                           0.0                 0.0                    1.0   \n",
       "2                           0.0                 0.0                    1.0   \n",
       "3                           0.0                 0.0                    1.0   \n",
       "4                           0.0                 0.0                    1.0   \n",
       "...                         ...                 ...                    ...   \n",
       "13031                       1.0                 0.0                    0.0   \n",
       "13032                       1.0                 0.0                    0.0   \n",
       "13033                       1.0                 0.0                    0.0   \n",
       "13034                       1.0                 0.0                    0.0   \n",
       "13035                       1.0                 0.0                    0.0   \n",
       "\n",
       "       education_College degree  education_Graduate degree  \\\n",
       "0                           0.0                        0.0   \n",
       "1                           0.0                        0.0   \n",
       "2                           0.0                        0.0   \n",
       "3                           0.0                        0.0   \n",
       "4                           0.0                        0.0   \n",
       "...                         ...                        ...   \n",
       "13031                       1.0                        0.0   \n",
       "13032                       1.0                        0.0   \n",
       "13033                       1.0                        0.0   \n",
       "13034                       1.0                        0.0   \n",
       "13035                       1.0                        0.0   \n",
       "\n",
       "       education_High school diploma or equivalent  \\\n",
       "0                                              1.0   \n",
       "1                                              1.0   \n",
       "2                                              1.0   \n",
       "3                                              1.0   \n",
       "4                                              1.0   \n",
       "...                                            ...   \n",
       "13031                                          0.0   \n",
       "13032                                          0.0   \n",
       "13033                                          0.0   \n",
       "13034                                          0.0   \n",
       "13035                                          0.0   \n",
       "\n",
       "       education_Less than a high school diploma  education_Other  \n",
       "0                                            0.0              0.0  \n",
       "1                                            0.0              0.0  \n",
       "2                                            0.0              0.0  \n",
       "3                                            0.0              0.0  \n",
       "4                                            0.0              0.0  \n",
       "...                                          ...              ...  \n",
       "13031                                        0.0              0.0  \n",
       "13032                                        0.0              0.0  \n",
       "13033                                        0.0              0.0  \n",
       "13034                                        0.0              0.0  \n",
       "13035                                        0.0              0.0  \n",
       "\n",
       "[13036 rows x 36 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_one_hot = pd.concat(\n",
    "    [\n",
    "        data_df.drop(columns=annotators_categorical_features),\n",
    "        pd.DataFrame(\n",
    "            one_hot_encoder.fit_transform(data_df[annotators_categorical_features]),\n",
    "            columns=one_hot_encoder.get_feature_names_out()\n",
    "        )\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "data_df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85273404-0224-40e2-96fb-f92d5034a70b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gender_Man', 'gender_Non-binary', 'gender_Woman',\n",
       "       'race_Arab American', 'race_Asian',\n",
       "       'race_Black or African American', 'race_Hispanic or Latino',\n",
       "       'race_Native American', 'race_White', 'age_18-24', 'age_25-29',\n",
       "       'age_30-34', 'age_35-39', 'age_40-44', 'age_45-49', 'age_50-54',\n",
       "       'age_54-59', 'age_60-64', 'age_>65', 'occupation_Employed',\n",
       "       'occupation_Homemaker', 'occupation_Other',\n",
       "       'occupation_Prefer not to disclose', 'occupation_Retired',\n",
       "       'occupation_Self-employed', 'occupation_Student',\n",
       "       'occupation_Unemployed', 'education_College degree',\n",
       "       'education_Graduate degree',\n",
       "       'education_High school diploma or equivalent',\n",
       "       'education_Less than a high school diploma', 'education_Other'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
