{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e6575d-ef06-4898-95db-9ff9570f5c43",
   "metadata": {},
   "source": [
    "# Training the SepHeads model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d0dd6-0385-475b-a9d7-4ec6027b8037",
   "metadata": {},
   "source": [
    "__Objective:__ train the SepHeads model (pre-trained DeBERTa text encoder, annotator-specific classification heads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00fff63-5b84-49cc-b6d9-f709f8156e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from custom_logger import get_logger\n",
    "from data_utils import subsample_dataset\n",
    "from model_utils import get_deberta_model\n",
    "from models import DebertaWithAnnotatorHeads\n",
    "from training_metrics import compute_metrics_sklearn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = get_logger('sepheads_model_training')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb7a367-3b08-4e8a-9117-92afbfed817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 09:58:29,363 - sepheads_model_training - INFO - Loading data from: {'train': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv', 'test': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv', 'annotators_data': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/annotators_data.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "DATASET_NAME = 'kumar'\n",
    "\n",
    "DATASET_PATHS = {\n",
    "    'popquorn': '../data/samples/POPQUORN_offensiveness.csv',\n",
    "    'kumar': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv',\n",
    "        # 'train':  '/data/milanlp/moscato/personal_hate_bounds_data/kumar_processed_with_ID_and_full_perspective_clean.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv',\n",
    "        'annotators_data': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/annotators_data.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.info(f'Loading data from: {DATASET_PATHS[DATASET_NAME]}')\n",
    "\n",
    "training_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['train']).drop(columns=['extreme_annotator'])\n",
    "test_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['test']).drop(columns=['extreme_annotator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259f269a-3bd6-49e1-9d35-dafcc37d5d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 09:58:30,598 - sepheads_model_training - INFO - Subsampling data to 10000 training datapoints\n",
      "2025-03-04 09:58:30,620 - sepheads_model_training - INFO - Optimal N datapoints: 10000 | Optimal N annotators: 81\n",
      "2025-03-04 09:58:30,621 - sepheads_model_training - INFO - Subsampling the data (manually including all extreme annotators)\n",
      "2025-03-04 09:58:30,635 - sepheads_model_training - INFO - N training datapoints: 12658 | N annotators: 236\n",
      "2025-03-04 09:58:30,645 - sepheads_model_training - INFO - N annotators: 236 | N training samples: 12658 | N test samples: 3028\n"
     ]
    }
   ],
   "source": [
    "N_ANNOTATORS_TEST = None\n",
    "OPTIMAL_N_TRAINING_DATAPOINTS = 10000\n",
    "\n",
    "if N_ANNOTATORS_TEST is not None:\n",
    "    logger.warning(f'Testing with {N_ANNOTATORS_TEST} annotators')\n",
    "\n",
    "    annotator_ids = sorted(training_data['annotator_id'].unique())[:N_ANNOTATORS_TEST]\n",
    "\n",
    "    training_data = training_data[training_data['annotator_id'].isin(annotator_ids)].reset_index(drop=True)\n",
    "    test_data = test_data[test_data['annotator_id'].isin(annotator_ids)].reset_index(drop=True)\n",
    "else:\n",
    "    if OPTIMAL_N_TRAINING_DATAPOINTS is not None:\n",
    "        logger.info(\n",
    "            f'Subsampling data to {OPTIMAL_N_TRAINING_DATAPOINTS}'\n",
    "            ' training datapoints'\n",
    "        )\n",
    "\n",
    "        training_data, test_data = subsample_dataset(\n",
    "            training_data,\n",
    "            test_data,\n",
    "            OPTIMAL_N_TRAINING_DATAPOINTS,\n",
    "            DATASET_PATHS[DATASET_NAME]['annotators_data']\n",
    "        )\n",
    "\n",
    "    annotator_ids = sorted(training_data['annotator_id'].unique())\n",
    "\n",
    "logger.info(\n",
    "    f'N annotators: {len(annotator_ids)} | N training samples: {len(training_data)}'\n",
    "    f' | N test samples: {len(test_data)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63c40bc-e064-4fcd-bace-4420c39c00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 09:59:15,616 - sepheads_model_training - INFO - Instantiating the SepHeads model\n",
      "2025-03-04 09:59:15,618 - sepheads_model_training - INFO - N labels found in training data: 2\n",
      "2025-03-04 09:59:15,618 - sepheads_model_training - INFO - Instantiating DeBERTa tokenizer\n",
      "2025-03-04 09:59:16,110 - sepheads_model_training - INFO - Instantiating DeBERTa model with default classification head\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the DeBERTa text encoder.\n",
    "logger.info('Instantiating the SepHeads model')\n",
    "\n",
    "num_labels = training_data['toxic_score'].unique().shape[0]\n",
    "\n",
    "model_dir = '/data1/shared_models/'\n",
    "\n",
    "logger.info(f'N labels found in training data: {num_labels}')\n",
    "\n",
    "deberta_tokenizer, deberta_model = get_deberta_model(\n",
    "    num_labels,\n",
    "    model_dir,\n",
    "    device,\n",
    "    use_custom_head=False,\n",
    "    pooler_out_features=768,  # Default: 768.\n",
    "    pooler_drop_prob=0.0,  # Default: 0.0\n",
    "    classifier_drop_prob=0.1,  # Default: 0.1\n",
    "    use_fast_tokenizer=False\n",
    ")\n",
    "\n",
    "deberta_with_annotator_heads_model = DebertaWithAnnotatorHeads(\n",
    "    deberta_encoder=deepcopy(deberta_model.deberta),\n",
    "    deberta_pooler=deepcopy(deberta_model.pooler),\n",
    "    deberta_dropout=deepcopy(deberta_model.dropout),\n",
    "    num_labels=num_labels,\n",
    "    annotator_ids=annotator_ids,\n",
    ")\n",
    "\n",
    "del deberta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37117cd8-49d8-4aec-b68a-445ba38e0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return deberta_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        # return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497966b6-ef34-4d58-9609-97f79a6ad6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:36:18,857 - sepheads_model_training - INFO - Creating tokenized datasets\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11470/11470 [00:03<00:00, 3287.23 examples/s]\n",
      "Flattening the indices: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11470/11470 [00:00<00:00, 22176.80 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2904/2904 [00:00<00:00, 3135.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create tokenized datasets.\n",
    "logger.info('Creating tokenized datasets')\n",
    "\n",
    "tokenized_training_data = (\n",
    "    # Create datast object from the DataFrame.\n",
    "    datasets.Dataset.from_dict(\n",
    "        training_data[[\n",
    "            'comment',\n",
    "            'toxic_score',\n",
    "            'annotator_id'\n",
    "        ]].rename(\n",
    "            columns={\n",
    "                'comment': 'text',\n",
    "                'toxic_score': 'label',\n",
    "                'annotator_id': 'annotator_ids',\n",
    "            }\n",
    "        )\n",
    "        .to_dict(orient='list')\n",
    "    )\n",
    "    # Tokenize.\n",
    "    .map(tokenize_function, batched=True)\n",
    "    # Remove useless column.\n",
    "    .remove_columns(\"text\")\n",
    "    .shuffle()\n",
    "    .flatten_indices()\n",
    ")\n",
    "\n",
    "tokenized_test_data = (\n",
    "    # Create datast object from the DataFrame.\n",
    "    datasets.Dataset.from_dict(\n",
    "        test_data[[\n",
    "            'comment',\n",
    "            'toxic_score',\n",
    "            'annotator_id'\n",
    "        ]].rename(\n",
    "            columns={\n",
    "                'comment': 'text',\n",
    "                'toxic_score': 'label',\n",
    "                'annotator_id': 'annotator_ids',\n",
    "            }\n",
    "        )\n",
    "        .to_dict(orient='list')\n",
    "    )\n",
    "    # Tokenize.\n",
    "    .map(tokenize_function, batched=True)\n",
    "    # Remove useless column.\n",
    "    .remove_columns(\"text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed9dbe3-d0de-43bc-a8cd-9847b1bdf392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_416901/3345286892.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_ID = 'sepheads_model_training_test'\n",
    "MODEL_OUTPUT_DIR = f'/data1/moscato/personalised-hate-boundaries-data/models/{EXPERIMENT_ID}/'\n",
    "N_EPOCHS = 5\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Options: 'no', 'epoch', 'steps' (requires the `save_steps` argument to be set though).\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=16,  # Default: 8.\n",
    "    gradient_accumulation_steps=1,  # Default: 1.\n",
    "    per_device_eval_batch_size=32,  # Default: 8.\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.0,  # For linear warmup of learning rate.\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    # label_names=list(roberta_classifier.config.id2label.keys()),\n",
    "    logging_strategy='epoch',\n",
    "    logging_first_step=True,\n",
    "    logging_dir=f'../tensorboard_logs/{EXPERIMENT_ID}/',\n",
    "    # logging_steps=10,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=deberta_tokenizer)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=deberta_with_annotator_heads_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=tokenized_test_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=deberta_tokenizer,\n",
    "    compute_metrics=compute_metrics_sklearn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4de3270-4874-43b3-8c00-fcaec018dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1795' max='1795' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1795/1795 29:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.662616</td>\n",
       "      <td>0.608471</td>\n",
       "      <td>0.577944</td>\n",
       "      <td>0.581421</td>\n",
       "      <td>0.577200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.643827</td>\n",
       "      <td>0.633953</td>\n",
       "      <td>0.602990</td>\n",
       "      <td>0.608768</td>\n",
       "      <td>0.601591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.635695</td>\n",
       "      <td>0.635331</td>\n",
       "      <td>0.602119</td>\n",
       "      <td>0.609538</td>\n",
       "      <td>0.600776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.632099</td>\n",
       "      <td>0.635675</td>\n",
       "      <td>0.601713</td>\n",
       "      <td>0.609698</td>\n",
       "      <td>0.600411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.616100</td>\n",
       "      <td>0.630548</td>\n",
       "      <td>0.637052</td>\n",
       "      <td>0.603566</td>\n",
       "      <td>0.611381</td>\n",
       "      <td>0.602184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
