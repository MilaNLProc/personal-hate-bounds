{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3b7e39-cfb2-4306-8b79-6b04be459d35",
   "metadata": {},
   "source": [
    "# Majority vote model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52649dc-4a4e-4749-9bf8-12d9aea8d4be",
   "metadata": {},
   "source": [
    "__Objective:__ develop a model for toxicity prediction on text trained with labels aggregated over annotators by majority vote (**no annotator modelling**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97429a23-f0db-4038-bef6-3e7cc10bdd11",
   "metadata": {},
   "source": [
    "**Number of training steps:**\n",
    "- The number of training steps depends on:\n",
    "    - Number of training samples `n_training_samples`.\n",
    "    - Batch size (per device) (`per_device_train_batch_size` parameter in Hugging Face's `Transformers` `TrainingArguments` object).\n",
    "    - Number of devices `n_devices` (by default the maximum number of accessible devices, if using the Hugging Face `Trainer`).\n",
    "    - Number of epochs `n_epochs`.\n",
    "- Formula: `n_steps = (n_training_samples / (n_devices * per_device_train_batch_size)) * n_epochs`.\n",
    "\n",
    "**Number of evaluation steps:**\n",
    "- There's more than one step only if the test (eval) set is big enough to require batching (with batch size given by the `per_device_eval_batch_size` parameter of the `TrainingArguments` object).\n",
    "- The formula is the same, but there's no concept of epoch (a single pass thorugh the whole test dataset is performed every time the test metrics are computed): `n_steps = n_test_samples / (n_devices * per_device_eval_batch_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3823f1f0-86ac-4d2e-8909-96a4d782f46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (AutoConfig, PretrainedConfig, AutoTokenizer, RobertaForSequenceClassification,\n",
    "    pipeline, DebertaForSequenceClassification, AutoModelForSequenceClassification)\n",
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout\n",
    "import datasets\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from custom_logger import get_logger\n",
    "from model_utils import freeze_model_weights\n",
    "from data_utils import generate_aggregated_labels_dataset\n",
    "from model_utils import get_deberta_model\n",
    "from training import WeightedLossTrainer\n",
    "from training_metrics import compute_metrics, compute_metrics_sklearn\n",
    "\n",
    "logger = get_logger('majority_vote_fine_tuning')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60deb0-e5ef-4887-aaf9-d52adf642ff0",
   "metadata": {},
   "source": [
    "## Load data and aggregate labels by majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1980fb4-77a2-463f-9e7f-fbc7498738df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'popquorn': '../data/samples/POPQUORN_offensiveness.csv',\n",
    "    'kumar': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv',\n",
    "        # 'train':  '/data/milanlp/moscato/personal_hate_bounds_data/kumar_processed_with_ID_and_full_perspective_clean.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv',\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASET_NAME = 'kumar'\n",
    "BINARIZE_LABELS = True\n",
    "SUBSAMPLE_MAJORITY_CLASS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930ab367-8cb9-4237-a72d-caf457b246e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 23:34:55,307 - majority_vote_fine_tuning - INFO - Reading kumar training data from: /data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv | Reading kumar test data from: /data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84817 21198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just a matter of time before pick up on this s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this is QUINN you DUMBASS ðŸ˜­ðŸ˜­ðŸ˜­</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I like Maxi, long term for sure. Just wouldnâ€™t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>anna really out there embarrassing amber, iâ€™d ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mfw we need to purge the system</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84812</th>\n",
       "      <td>106029</td>\n",
       "      <td>\"Harvey!\\\" He shouted. \\\"Gather up these goyim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84813</th>\n",
       "      <td>106031</td>\n",
       "      <td>Precisely. Drug testing does fuck-all to ensur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84814</th>\n",
       "      <td>106032</td>\n",
       "      <td>Adult women I know are generally smarter than ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84815</th>\n",
       "      <td>106033</td>\n",
       "      <td>This is as stupid as saying having a 3rd degre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84816</th>\n",
       "      <td>106034</td>\n",
       "      <td>\"Boo hoo, Tharizdun wants me to be his avatar!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84817 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_id                                               text  label\n",
       "0            0  Just a matter of time before pick up on this s...      0\n",
       "1            1                      this is QUINN you DUMBASS ðŸ˜­ðŸ˜­ðŸ˜­      1\n",
       "2            2  I like Maxi, long term for sure. Just wouldnâ€™t...      0\n",
       "3            3  anna really out there embarrassing amber, iâ€™d ...      1\n",
       "4            4                    mfw we need to purge the system      0\n",
       "...        ...                                                ...    ...\n",
       "84812   106029  \"Harvey!\\\" He shouted. \\\"Gather up these goyim...      1\n",
       "84813   106031  Precisely. Drug testing does fuck-all to ensur...      1\n",
       "84814   106032  Adult women I know are generally smarter than ...      0\n",
       "84815   106033  This is as stupid as saying having a 3rd degre...      1\n",
       "84816   106034  \"Boo hoo, Tharizdun wants me to be his avatar!...      1\n",
       "\n",
       "[84817 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, test_data = generate_aggregated_labels_dataset(\n",
    "    DATASET_NAME,\n",
    "    DATASET_PATHS[DATASET_NAME]['train'],\n",
    "    DATASET_PATHS[DATASET_NAME]['test'],\n",
    "    subsample_majority_class=SUBSAMPLE_MAJORITY_CLASS\n",
    ")\n",
    "\n",
    "print(len(training_data), len(test_data))\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01d4c9-5923-4f99-81ae-7696a8fb124e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9bbf6d-af6b-4a2a-90c7-2316ed9db63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bbef2-7566-4d82-9587-bfe34c119639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c38b6f-144c-40cd-9fe4-cdd7d0209ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e99122-a79d-417f-99fa-790e217a423f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.False_)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'].isna().any(), test_data['label'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5941c9c5-12da-418d-a251-13b7c9573504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.4703420304891708), np.float64(0.46419473535239175))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'].mean(), test_data['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4be56e-b8b9-4f67-b43a-b521c604be53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.False_)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.duplicated().any(), test_data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30313ba7-183c-4103-bf55-f29376f6de14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restricted_data = True\n",
    "\n",
    "if restricted_data:\n",
    "    train_ds = datasets.Dataset.from_dict(\n",
    "        training_data\n",
    "        .iloc[:10000]  # For testing!\n",
    "        .to_dict(orient='list')\n",
    "    )\n",
    "    test_ds = datasets.Dataset.from_dict(\n",
    "        test_data\n",
    "        .iloc[:1000]  # For testing!\n",
    "        .to_dict(orient='list')\n",
    "    )\n",
    "    \n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261d860d-a5ec-4b2a-9ad2-5db93e8299ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.401), np.float64(0.401))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([sample['label'] for sample in train_ds]), np.mean([sample['label'] for sample in test_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d305852-94f3-4c6b-9a3d-03889db62504",
   "metadata": {},
   "source": [
    "## Load encoder-only model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf6556-7d61-4380-97fc-6fb5212ecc5b",
   "metadata": {},
   "source": [
    "Pretrained encoder, newly initialized classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d85c13-3242-49cc-bbc3-d969bc88ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 15:13:23,364 - majority_vote_fine_tuning - INFO - Instantiating DeBERTa tokenizer\n",
      "2025-02-03 15:13:23,963 - majority_vote_fine_tuning - INFO - Instantiating DeBERTa model with default classification head\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = training_data['label'].unique().shape[0]\n",
    "\n",
    "tokenizer, classifier = get_deberta_model(\n",
    "    num_labels,\n",
    "    # '/data/milanlp/huggingface/hub/',\n",
    "    '/data1/shared_models/',\n",
    "    device,\n",
    "    use_custom_head=False,\n",
    "    pooler_out_features=768,\n",
    "    pooler_drop_prob=0.,\n",
    "    classifier_drop_prob=0.1,\n",
    "    use_fast_tokenizer=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660edbc3-83ab-4565-8ad7-046be5bd89ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5c6815f-0ebe-4532-8b4d-d873519f0eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = 'roberta-base'\n",
    "\n",
    "# num_labels = majority_vote_data_df['label'].unique().shape[0]\n",
    "\n",
    "# # # Config for the encoder.\n",
    "# roberta_classifier_config = AutoConfig.from_pretrained(\n",
    "#     model_id,\n",
    "#     finetuning_task=\"text-classification\",\n",
    "#     id2label={\n",
    "#         i: label\n",
    "#         for i, label in enumerate(range(num_labels))\n",
    "#     },\n",
    "#     label2id={\n",
    "#         label: i\n",
    "#         for i, label in enumerate(range(num_labels))\n",
    "#     }\n",
    "#     # id2label={\n",
    "#     #     i: int(label)\n",
    "#     #     for i, label in enumerate(majority_vote_data_df['offensiveness'].unique())\n",
    "#     # },\n",
    "#     # label2id={\n",
    "#     #     int(label): i\n",
    "#     #     for i, label in enumerate(majority_vote_data_df['offensiveness'].unique())\n",
    "#     # }\n",
    "# )\n",
    "\n",
    "# # Config for the classification head. These are all the\n",
    "# # parameters a `RobertaClassificationHead` requires.\n",
    "# roberta_classification_head_config = PretrainedConfig()\n",
    "\n",
    "# roberta_classification_head_config.classifier_dropout = 0.1\n",
    "# roberta_classification_head_config.hidden_size = 64\n",
    "# roberta_classification_head_config.num_labels = majority_vote_data_df['label'].unique().shape[0]\n",
    "\n",
    "\n",
    "# logger.info('Instantiating tokenizer, classification model and pipeline')\n",
    "\n",
    "# # Instantiate tokenizer.\n",
    "# roberta_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# # Instantiate RoBERTa model.\n",
    "# roberta_classifier = RobertaForSequenceClassification.from_pretrained(\n",
    "#     model_id,\n",
    "#     config=roberta_classifier_config,\n",
    "# )\n",
    "\n",
    "# # Substitute the default classification head with a custom one.\n",
    "# classification_head = RobertaClassificationHead(roberta_classification_head_config)\n",
    "# classification_head.dense = torch.nn.Linear(\n",
    "#     roberta_classifier.config.hidden_size,  # The `in_features` parameter must be equal to the encoder's hidden size.\n",
    "#     roberta_classification_head_config.hidden_size,\n",
    "# )\n",
    "\n",
    "# roberta_classifier.classifier = classification_head\n",
    "\n",
    "\n",
    "# # Put everything together in a single pipeline object.\n",
    "# roberta_classifier_pipeline = pipeline(\n",
    "#     task='text-classification',\n",
    "#     config=roberta_classifier_config,\n",
    "#     tokenizer=roberta_tokenizer,\n",
    "#     model=roberta_classifier,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bd2a6-b891-4057-9a6a-aae8b51990c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd6f3eb-0c20-4a1a-a135-92f416b1ffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SequenceClassifierOutput(loss=tensor(0.6900, device='cuda:0'), logits=tensor([[-0.0746,  0.0038],\n",
       "         [-0.0785,  0.0243],\n",
       "         [-0.0770,  0.0042],\n",
       "         [-0.0737,  0.0166]], device='cuda:0'), hidden_states=None, attentions=None),\n",
       " tensor([1, 1, 1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test.\n",
    "with torch.no_grad():\n",
    "    output = classifier(**dict(\n",
    "        **tokenizer(\n",
    "            training_data['text'].iloc[:4].tolist(),\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device=device),\n",
    "        **{'labels': torch.LongTensor(training_data['label'].iloc[:4]).to(device=device)}\n",
    "    ))\n",
    "\n",
    "output, torch.argmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fb3e3-4c6f-4701-b12c-dfbd171dcf7e",
   "metadata": {},
   "source": [
    "Tokenize datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d19c5cf7-c8d1-4db2-a257-27b6348a0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "        # return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55675070-1bc8-45f6-b3bb-0938515ef6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 15:13:27,694 - majority_vote_fine_tuning - INFO - Tokenizing datasets\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:02<00:00, 4119.25 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 4019.36 examples/s]\n",
      "2025-02-03 15:13:31,749 - majority_vote_fine_tuning - INFO - Training dataset size: 10000 | Test dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets.\n",
    "logger.info(f'Tokenizing datasets')\n",
    "\n",
    "tokenized_train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "logger.info(f'Training dataset size: {len(train_ds)} | Test dataset size: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "530d0882-1ee6-414e-b005-93ad1683aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should this be passed to the trainer?\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcdc2eec-3cdc-40d0-b874-eefedf5c3a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79480876-1dc4-4961-b389-432754d83543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.401), np.float64(0.401))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([s['label'] for s in tokenized_train_ds]), np.mean([s['label'] for s in tokenized_test_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1e924-720f-4f5f-a776-0bdd03d535cc",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e91276cf-c4f6-404a-ab66-c97e8f6d45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 15:13:37,900 - majority_vote_fine_tuning - INFO - N params: 184423682 | N trainable params: 184423682\n",
      "2025-02-03 15:13:37,901 - majority_vote_fine_tuning - INFO - Training mode selected: True\n"
     ]
    }
   ],
   "source": [
    "FREEZE_ENCODER_PARAMS = False\n",
    "\n",
    "if FREEZE_ENCODER_PARAMS:\n",
    "    freeze_model_weights(classifier_pipeline.model, trainable_modules=['classifier'])\n",
    "\n",
    "n_params_total = sum([p.numel() for p in classifier.parameters()])\n",
    "n_params_trainable = sum([p.numel() for p in classifier.parameters() if p.requires_grad])\n",
    "\n",
    "logger.info(\n",
    "    f'N params: {n_params_total} | N trainable params: {n_params_trainable}'\n",
    ")\n",
    "\n",
    "classifier.train()\n",
    "\n",
    "logger.info(\n",
    "    f'Training mode selected: {classifier.training}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8869c869-d01a-4eb3-8d13-a016abe2287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 'majority_vote_model_new_binarized_labels_restricted_data_3'\n",
    "MODEL_OUTPUT_DIR = f'/data1/moscato/personalised-hate-boundaries-data/models/{EXPERIMENT_ID}/'\n",
    "N_EPOCHS = 5\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Options: 'no', 'epoch', 'steps' (requires the `save_steps` argument to be set though).\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=16,  # Default: 8.\n",
    "    gradient_accumulation_steps=1,  # Default: 1.\n",
    "    per_device_eval_batch_size=32,  # Default: 8.\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.0,  # For linear warmup of learning rate.\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    # label_names=list(roberta_classifier.config.id2label.keys()),\n",
    "    logging_strategy='epoch',\n",
    "    logging_first_step=True,\n",
    "    logging_dir=f'../tensorboard_logs/{EXPERIMENT_ID}/',\n",
    "    # logging_steps=10,\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b12d6134-63a1-46f2-b235-a45e0786c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 15:14:11,809 - majority_vote_fine_tuning - INFO - Training without class weights\n",
      "/tmp/ipykernel_189186/4263755792.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "CLASS_WEIGHTS = False\n",
    "\n",
    "if CLASS_WEIGHTS:\n",
    "    logger.info('Training with custom class weights')\n",
    "\n",
    "    class_weights_from_frequencies = (\n",
    "        majority_vote_data_df.groupby('label')['text_id'].count().sort_index(ascending=True)\n",
    "        / len(majority_vote_data_df)\n",
    "    ).to_list()\n",
    "    \n",
    "    trainer = WeightedLossTrainer(\n",
    "        class_weights=torch.tensor(class_weights_from_frequencies).to(device=device),\n",
    "        model=classifier,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_ds,\n",
    "        eval_dataset=tokenized_test_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_sklearn,\n",
    "    )\n",
    "else:\n",
    "    logger.info('Training without class weights')\n",
    "    \n",
    "    trainer = transformers.Trainer(\n",
    "        model=classifier,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_ds,\n",
    "        eval_dataset=tokenized_test_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_sklearn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfb36fc9-0f86-4b89-ac59-20362e1c31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 22:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.521294</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.715247</td>\n",
       "      <td>0.713696</td>\n",
       "      <td>0.719724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.506350</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.738154</td>\n",
       "      <td>0.746172</td>\n",
       "      <td>0.734070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.498527</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.737298</td>\n",
       "      <td>0.736071</td>\n",
       "      <td>0.738933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.511377</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.733353</td>\n",
       "      <td>0.734573</td>\n",
       "      <td>0.732328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.514345</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.729331</td>\n",
       "      <td>0.728586</td>\n",
       "      <td>0.730205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "370e7cda-cbb6-4a41-ad74-cda05204a0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=0.19233009474618093, metrics={'train_runtime': 63.1345, 'train_samples_per_second': 15.839, 'train_steps_per_second': 1.109, 'total_flos': 263115773952000.0, 'train_loss': 0.19233009474618093, 'epoch': 10.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbd0a9-561e-445c-895d-7aeaa6d52658",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbed914-a282-41b2-8b5a-1964a86f9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval metrics.\n",
    "pd.DataFrame([state for state in trainer.state.log_history if 'eval_loss' in state.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b7c8b-8762-4bdc-bc28-1442f13ea9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef871633-698c-492f-a25a-86cb29df3c02",
   "metadata": {},
   "source": [
    "## Check: manually reproduce the metrics seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5862bbf-c7b4-4be7-9b0e-ed864b191f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da6661-cc19-4314-89d0-571d382d78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_steps = 20\n",
    "\n",
    "classifier_loaded = AutoModelForSequenceClassification.from_pretrained(\n",
    "    os.path.join(MODEL_OUTPUT_DIR, f'checkpoint-{checkpoint_steps}/')\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00d2a6-3714-423b-aef7-d8f4a5f3c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, pl in zip(classifier.parameters(), classifier_loaded.parameters()):\n",
    "    try:\n",
    "        assert (p == pl).all()\n",
    "    except AssertionError:\n",
    "        raise AssertionError(\n",
    "            f\"Loaded model's parameters (checkpoint {checkpoint_steps}) are different from the instantiated one's\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5d60f-318b-48ed-b304-c739518093a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13263b-fc55-4bd3-85c0-5a2063e22a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d34477-0ae4-42f5-b050-08d5c0deef25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7ffd5-e87a-476a-84a5-8ed3f75db926",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_samples = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_deberta_output = classifier_loaded(\n",
    "        input_ids=torch.tensor(tokenized_test_ds['input_ids'])[:n_test_samples, ...].to(device=device),\n",
    "        attention_mask=torch.tensor(tokenized_test_ds['attention_mask'])[:n_test_samples, ...].to(device=device),\n",
    "        token_type_ids=torch.tensor(tokenized_test_ds['token_type_ids'])[:n_test_samples, ...].to(device=device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8d5f0-f227-4ad0-a083-e50a61cff9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeEvalPred:\n",
    "    def __init__(self, logits, labels):\n",
    "        self.predictions = logits\n",
    "        self.label_ids = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7c11f-4e22-454b-a6f7-841519064e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fep = FakeEvalPred(\n",
    "    logits=test_deberta_output.logits.cpu().numpy(),\n",
    "    labels=torch.tensor(tokenized_test_ds['label'])[:n_test_samples, ...].cpu().numpy()\n",
    ")\n",
    "\n",
    "compute_metrics_sklearn(fep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
