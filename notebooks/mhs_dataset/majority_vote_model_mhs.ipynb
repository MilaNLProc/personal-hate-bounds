{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3b7e39-cfb2-4306-8b79-6b04be459d35",
   "metadata": {},
   "source": [
    "# Majority vote model for the MHS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52649dc-4a4e-4749-9bf8-12d9aea8d4be",
   "metadata": {},
   "source": [
    "__Objective:__ develop a model for toxicity prediction on text trained with labels aggregated over annotators by majority vote (**no annotator modelling**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97429a23-f0db-4038-bef6-3e7cc10bdd11",
   "metadata": {},
   "source": [
    "**Number of training steps:**\n",
    "- The number of training steps depends on:\n",
    "    - Number of training samples `n_training_samples`.\n",
    "    - Batch size (per device) (`per_device_train_batch_size` parameter in Hugging Face's `Transformers` `TrainingArguments` object).\n",
    "    - Number of devices `n_devices` (by default the maximum number of accessible devices, if using the Hugging Face `Trainer`).\n",
    "    - Number of epochs `n_epochs`.\n",
    "- Formula: `n_steps = (n_training_samples / (n_devices * per_device_train_batch_size)) * n_epochs`.\n",
    "\n",
    "**Number of evaluation steps:**\n",
    "- There's more than one step only if the test (eval) set is big enough to require batching (with batch size given by the `per_device_eval_batch_size` parameter of the `TrainingArguments` object).\n",
    "- The formula is the same, but there's no concept of epoch (a single pass thorugh the whole test dataset is performed every time the test metrics are computed): `n_steps = n_test_samples / (n_devices * per_device_eval_batch_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3823f1f0-86ac-4d2e-8909-96a4d782f46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (AutoConfig, PretrainedConfig, AutoTokenizer, RobertaForSequenceClassification,\n",
    "    pipeline, DebertaForSequenceClassification, AutoModelForSequenceClassification)\n",
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout\n",
    "import datasets\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from custom_logger import get_logger\n",
    "from model_utils import freeze_model_weights\n",
    "from data_utils import generate_aggregated_labels_dataset\n",
    "from model_utils import get_deberta_model\n",
    "from training import WeightedLossTrainer\n",
    "from training_metrics import compute_metrics, compute_metrics_sklearn\n",
    "\n",
    "logger = get_logger('majority_vote_fine_tuning_mhs')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60deb0-e5ef-4887-aaf9-d52adf642ff0",
   "metadata": {},
   "source": [
    "## Load data and aggregate labels by majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1980fb4-77a2-463f-9e7f-fbc7498738df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'popquorn': '../data/samples/POPQUORN_offensiveness.csv',\n",
    "    'kumar': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv',\n",
    "        # 'train':  '/data/milanlp/moscato/personal_hate_bounds_data/kumar_processed_with_ID_and_full_perspective_clean.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv',\n",
    "    },\n",
    "    'mhs': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_train.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_test.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASET_NAME = 'mhs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d329ad9f-68f2-4977-a8a4-dfe4099addab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1595, 2), (410, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['train'])\n",
    "test_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['test'])\n",
    "\n",
    "# Aggregate by majority vote.\n",
    "training_data = training_data.groupby('text_id').agg(\n",
    "    text=pd.NamedAgg('text', 'first'),\n",
    "    label=pd.NamedAgg(\n",
    "        'toxic_score',\n",
    "        lambda group: group.value_counts(ascending=False).index[0]\n",
    "    )\n",
    ").reset_index().drop(columns=['text_id'])\n",
    "\n",
    "test_data = test_data.groupby('text_id').agg(\n",
    "    text=pd.NamedAgg('text', 'first'),\n",
    "    label=pd.NamedAgg(\n",
    "        'toxic_score',\n",
    "        lambda group: group.value_counts(ascending=False).index[0]\n",
    "    )\n",
    ").reset_index().drop(columns=['text_id'])\n",
    "\n",
    "training_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e99122-a79d-417f-99fa-790e217a423f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.False_)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'].isna().any(), test_data['label'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5941c9c5-12da-418d-a251-13b7c9573504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.2896551724137931), np.float64(0.28536585365853656))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'].mean(), test_data['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4be56e-b8b9-4f67-b43a-b521c604be53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.False_)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.duplicated().any(), test_data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30313ba7-183c-4103-bf55-f29376f6de14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595, 410)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = datasets.Dataset.from_dict(\n",
    "    training_data\n",
    "    .to_dict(orient='list')\n",
    ")\n",
    "test_ds = datasets.Dataset.from_dict(\n",
    "    test_data\n",
    "    .to_dict(orient='list')\n",
    ")\n",
    "    \n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d305852-94f3-4c6b-9a3d-03889db62504",
   "metadata": {},
   "source": [
    "## Load encoder-only model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf6556-7d61-4380-97fc-6fb5212ecc5b",
   "metadata": {},
   "source": [
    "Pretrained encoder, newly initialized classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d85c13-3242-49cc-bbc3-d969bc88ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:02:51,639 - majority_vote_fine_tuning_mhs - INFO - Instantiating DeBERTa tokenizer\n",
      "2025-05-05 14:02:52,375 - majority_vote_fine_tuning_mhs - INFO - Instantiating DeBERTa model with default classification head\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "num_labels = training_data['label'].unique().shape[0]\n",
    "\n",
    "tokenizer, classifier = get_deberta_model(\n",
    "    num_labels,\n",
    "    # '/data/milanlp/huggingface/hub/',\n",
    "    '/data1/shared_models/',\n",
    "    device,\n",
    "    use_custom_head=False,\n",
    "    pooler_out_features=768,\n",
    "    pooler_drop_prob=0.,\n",
    "    classifier_drop_prob=0.1,\n",
    "    use_fast_tokenizer=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd6f3eb-0c20-4a1a-a135-92f416b1ffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SequenceClassifierOutput(loss=tensor(0.6840, device='cuda:0'), logits=tensor([[-0.0376, -0.0624],\n",
       "         [-0.0422, -0.0827],\n",
       "         [-0.0383, -0.0800],\n",
       "         [-0.0459, -0.0788]], device='cuda:0'), hidden_states=None, attentions=None),\n",
       " tensor([0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test.\n",
    "with torch.no_grad():\n",
    "    output = classifier(**dict(\n",
    "        **tokenizer(\n",
    "            training_data['text'].iloc[:4].tolist(),\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device=device),\n",
    "        **{'labels': torch.LongTensor(training_data['label'].iloc[:4]).to(device=device)}\n",
    "    ))\n",
    "\n",
    "output, torch.argmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fb3e3-4c6f-4701-b12c-dfbd171dcf7e",
   "metadata": {},
   "source": [
    "Tokenize datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d19c5cf7-c8d1-4db2-a257-27b6348a0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "        # return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55675070-1bc8-45f6-b3bb-0938515ef6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:02:54,443 - majority_vote_fine_tuning_mhs - INFO - Tokenizing datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31b8970cbcc40beac368dec99a4dceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b5f55bfc3849f9b09acbc85346373e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:02:56,319 - majority_vote_fine_tuning_mhs - INFO - Training dataset size: 1595 | Test dataset size: 410\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets.\n",
    "logger.info(f'Tokenizing datasets')\n",
    "\n",
    "tokenized_train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "logger.info(f'Training dataset size: {len(train_ds)} | Test dataset size: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530d0882-1ee6-414e-b005-93ad1683aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should this be passed to the trainer?\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcdc2eec-3cdc-40d0-b874-eefedf5c3a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1595\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79480876-1dc4-4961-b389-432754d83543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.2896551724137931), np.float64(0.28536585365853656))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([s['label'] for s in tokenized_train_ds]), np.mean([s['label'] for s in tokenized_test_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1e924-720f-4f5f-a776-0bdd03d535cc",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e91276cf-c4f6-404a-ab66-c97e8f6d45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:03:05,863 - majority_vote_fine_tuning_mhs - INFO - N params: 184423682 | N trainable params: 184423682\n",
      "2025-05-05 14:03:05,865 - majority_vote_fine_tuning_mhs - INFO - Training mode selected: True\n"
     ]
    }
   ],
   "source": [
    "FREEZE_ENCODER_PARAMS = False\n",
    "\n",
    "if FREEZE_ENCODER_PARAMS:\n",
    "    freeze_model_weights(classifier_pipeline.model, trainable_modules=['classifier'])\n",
    "\n",
    "n_params_total = sum([p.numel() for p in classifier.parameters()])\n",
    "n_params_trainable = sum([p.numel() for p in classifier.parameters() if p.requires_grad])\n",
    "\n",
    "logger.info(\n",
    "    f'N params: {n_params_total} | N trainable params: {n_params_trainable}'\n",
    ")\n",
    "\n",
    "classifier.train()\n",
    "\n",
    "logger.info(\n",
    "    f'Training mode selected: {classifier.training}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8869c869-d01a-4eb3-8d13-a016abe2287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 'majority_vote_model_mhs_run_1'\n",
    "MODEL_OUTPUT_DIR = f'/data1/moscato/personalised-hate-boundaries-data/models/mhs/{EXPERIMENT_ID}/'\n",
    "N_EPOCHS = 10\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Options: 'no', 'epoch', 'steps' (requires the `save_steps` argument to be set though).\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=16,  # Default: 8.\n",
    "    gradient_accumulation_steps=1,  # Default: 1.\n",
    "    per_device_eval_batch_size=32,  # Default: 8.\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.0,  # For linear warmup of learning rate.\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    # label_names=list(roberta_classifier.config.id2label.keys()),\n",
    "    logging_strategy='epoch',\n",
    "    logging_first_step=True,\n",
    "    logging_dir=f'../tensorboard_logs/{EXPERIMENT_ID}/',\n",
    "    # logging_steps=10,\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b12d6134-63a1-46f2-b235-a45e0786c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:03:31,399 - majority_vote_fine_tuning_mhs - INFO - Training without class weights\n",
      "/tmp/ipykernel_574732/4263755792.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "CLASS_WEIGHTS = False\n",
    "\n",
    "if CLASS_WEIGHTS:\n",
    "    logger.info('Training with custom class weights')\n",
    "\n",
    "    class_weights_from_frequencies = (\n",
    "        majority_vote_data_df.groupby('label')['text_id'].count().sort_index(ascending=True)\n",
    "        / len(majority_vote_data_df)\n",
    "    ).to_list()\n",
    "    \n",
    "    trainer = WeightedLossTrainer(\n",
    "        class_weights=torch.tensor(class_weights_from_frequencies).to(device=device),\n",
    "        model=classifier,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_ds,\n",
    "        eval_dataset=tokenized_test_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_sklearn,\n",
    "    )\n",
    "else:\n",
    "    logger.info('Training without class weights')\n",
    "    \n",
    "    trainer = transformers.Trainer(\n",
    "        model=classifier,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_ds,\n",
    "        eval_dataset=tokenized_test_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_sklearn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfb36fc9-0f86-4b89-ac59-20362e1c31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 08:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.628900</td>\n",
       "      <td>0.561572</td>\n",
       "      <td>0.714634</td>\n",
       "      <td>0.416785</td>\n",
       "      <td>0.714634</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500100</td>\n",
       "      <td>0.427912</td>\n",
       "      <td>0.714634</td>\n",
       "      <td>0.416785</td>\n",
       "      <td>0.714634</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.390657</td>\n",
       "      <td>0.831707</td>\n",
       "      <td>0.751430</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.720530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.353668</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.838396</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.844535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.324887</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.838396</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.844535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>0.863415</td>\n",
       "      <td>0.832560</td>\n",
       "      <td>0.832560</td>\n",
       "      <td>0.832560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.333551</td>\n",
       "      <td>0.860976</td>\n",
       "      <td>0.835630</td>\n",
       "      <td>0.825867</td>\n",
       "      <td>0.848823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.336588</td>\n",
       "      <td>0.863415</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.829071</td>\n",
       "      <td>0.847962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.239800</td>\n",
       "      <td>0.342350</td>\n",
       "      <td>0.856098</td>\n",
       "      <td>0.830625</td>\n",
       "      <td>0.820165</td>\n",
       "      <td>0.845410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.342119</td>\n",
       "      <td>0.858537</td>\n",
       "      <td>0.831591</td>\n",
       "      <td>0.823416</td>\n",
       "      <td>0.841982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Bad pipe message: %s [b'\\xbe<j5\\x11\\xfa\\xb8/\\x00\\x8d\\xa5\\xe7J\\xb5{\\xebD\\xf8 \\x00\\x80wq\\x98\\xbe<\\xe8g9\\xb9>\\x1d$Z\\x9d\\x0b\\xbato\\xb4\\x14\\xa03\\xb6\\x9dY\\x9fD\\xd0\\n\\x0c\\x00\\x1a\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\t\\xc0\\x13\\xc0\\n\\xc0\\x14\\x13\\x01\\x13\\x02\\x13\\x03\\x01\\x00\\x05S\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\tlocalhost\\x00\\x0b\\x00\\x02\\x01\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x17\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x05\\x00\\x05']\n",
      "Bad pipe message: %s [b'']\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "370e7cda-cbb6-4a41-ad74-cda05204a0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=0.19233009474618093, metrics={'train_runtime': 63.1345, 'train_samples_per_second': 15.839, 'train_steps_per_second': 1.109, 'total_flos': 263115773952000.0, 'train_loss': 0.19233009474618093, 'epoch': 10.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbd0a9-561e-445c-895d-7aeaa6d52658",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbed914-a282-41b2-8b5a-1964a86f9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval metrics.\n",
    "pd.DataFrame([state for state in trainer.state.log_history if 'eval_loss' in state.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b7c8b-8762-4bdc-bc28-1442f13ea9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef871633-698c-492f-a25a-86cb29df3c02",
   "metadata": {},
   "source": [
    "## Check: manually reproduce the metrics seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5862bbf-c7b4-4be7-9b0e-ed864b191f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08da6661-cc19-4314-89d0-571d382d78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_steps = 200\n",
    "\n",
    "classifier_loaded = AutoModelForSequenceClassification.from_pretrained(\n",
    "    os.path.join(MODEL_OUTPUT_DIR, f'checkpoint-{checkpoint_steps}/')\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc00d2a6-3714-423b-aef7-d8f4a5f3c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check.\n",
    "for p, pl in zip(classifier.parameters(), classifier_loaded.parameters()):\n",
    "    try:\n",
    "        assert (p == pl).all()\n",
    "    except AssertionError:\n",
    "        raise AssertionError(\n",
    "            f\"Loaded model's parameters (checkpoint {checkpoint_steps}) are different from the instantiated one's\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75f5d60f-318b-48ed-b304-c739518093a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88749330051445658742de1061129c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred_logits = []\n",
    "\n",
    "for i, row in tqdm(test_data.iterrows()):\n",
    "    with torch.no_grad():\n",
    "        output = classifier(**dict(\n",
    "            **tokenizer(\n",
    "                row['text'],\n",
    "                return_tensors='pt',\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(device=device),\n",
    "            # **{'labels': torch.LongTensor(training_data['label'].iloc[:4]).to(device=device)}\n",
    "        ))\n",
    "\n",
    "    test_pred_logits.append(output.logits.cpu().numpy())\n",
    "\n",
    "test_pred_logits = np.concat(test_pred_logits)\n",
    "test_pred = np.argmax(test_pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d9d15a2-1858-43b9-a9c7-fad8d864923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       293\n",
      "           1       0.75      0.79      0.77       117\n",
      "\n",
      "    accuracy                           0.87       410\n",
      "   macro avg       0.83      0.84      0.84       410\n",
      "weighted avg       0.87      0.87      0.87       410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report on the MHS test data.\n",
    "print(classification_report(\n",
    "    y_true=test_data['label'].values,\n",
    "    y_pred=test_pred\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
