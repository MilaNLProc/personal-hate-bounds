{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e6575d-ef06-4898-95db-9ff9570f5c43",
   "metadata": {},
   "source": [
    "# Training the SepHeads model on the MHS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d0dd6-0385-475b-a9d7-4ec6027b8037",
   "metadata": {},
   "source": [
    "__Objective:__ train the SepHeads model (pre-trained DeBERTa text encoder, annotator-specific classification heads) on the MHS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00fff63-5b84-49cc-b6d9-f709f8156e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from custom_logger import get_logger\n",
    "from data_utils import subsample_dataset\n",
    "from model_utils import get_deberta_model\n",
    "from models import DebertaWithAnnotatorHeads, DebertaWithAnnotatorHeadsPretrained, DebertaWithAnnotatorHeadsPretrainedConfig\n",
    "from training_metrics import compute_metrics_sklearn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = get_logger('sepheads_model_training_mhs')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb7a367-3b08-4e8a-9117-92afbfed817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 19:06:51,582 - sepheads_model_training_mhs - INFO - Loading data from: {'train': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_train_10_samples_per_annotator.csv', 'test': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_test_10_samples_per_annotator.csv'}\n",
      "2025-05-12 19:06:51,640 - sepheads_model_training_mhs - INFO - N annotators: 2219 | N training samples: 21742 | N test samples: 9379\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "DATASET_NAME = 'mhs'\n",
    "\n",
    "DATASET_PATHS = {\n",
    "    'popquorn': '../data/samples/POPQUORN_offensiveness.csv',\n",
    "    'kumar': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_train.csv',\n",
    "        # 'train':  '/data/milanlp/moscato/personal_hate_bounds_data/kumar_processed_with_ID_and_full_perspective_clean.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/kumar_processed_with_ID_and_full_perspective_clean_test.csv',\n",
    "        'annotators_data': '/data1/moscato/personalised-hate-boundaries-data/data/kumar_perspective_clean/annotators_data.csv'\n",
    "    },\n",
    "    # Data used for the original training./\n",
    "    # 'mhs': {\n",
    "    #     'train': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_train.csv',\n",
    "    #     'test': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_test.csv'\n",
    "    # }\n",
    "    # New data (more samples, 10 samples per annotator.\n",
    "    'mhs': {\n",
    "        'train': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_train_10_samples_per_annotator.csv',\n",
    "        'test': '/data1/moscato/personalised-hate-boundaries-data/data/measuring_hate_speech_data_clean/mhs_clean_test_10_samples_per_annotator.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.info(f'Loading data from: {DATASET_PATHS[DATASET_NAME]}')\n",
    "\n",
    "training_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['train']).drop(columns=['extreme_annotator'])\n",
    "test_data = pd.read_csv(DATASET_PATHS[DATASET_NAME]['test']).drop(columns=['extreme_annotator'])\n",
    "\n",
    "annotator_ids = sorted(training_data['annotator_id'].unique())\n",
    "\n",
    "logger.info(\n",
    "    f'N annotators: {len(annotator_ids)} | N training samples: {len(training_data)}'\n",
    "    f' | N test samples: {len(test_data)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83047a1-32ee-40b5-acb4-759417fe7f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(7), np.int64(3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    training_data.groupby('annotator_id')['text_id'].count().min(),\n",
    "    test_data.groupby('annotator_id')['text_id'].count().min()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f281cfb3-1a18-4712-b004-4ad2d9d9282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.35525710606199984)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['toxic_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63c40bc-e064-4fcd-bace-4420c39c00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 19:07:09,139 - sepheads_model_training_mhs - INFO - Instantiating the SepHeads model\n",
      "2025-05-12 19:07:09,140 - sepheads_model_training_mhs - INFO - N labels found in training data: 2\n",
      "2025-05-12 19:07:09,140 - sepheads_model_training_mhs - INFO - Instantiating DeBERTa tokenizer\n",
      "2025-05-12 19:07:09,829 - sepheads_model_training_mhs - INFO - Instantiating DeBERTa model with default classification head\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the DeBERTa text encoder.\n",
    "logger.info('Instantiating the SepHeads model')\n",
    "\n",
    "num_labels = training_data['toxic_score'].unique().shape[0]\n",
    "\n",
    "model_dir = '/data1/shared_models/'\n",
    "\n",
    "logger.info(f'N labels found in training data: {num_labels}')\n",
    "\n",
    "deberta_tokenizer, deberta_model = get_deberta_model(\n",
    "    num_labels,\n",
    "    model_dir,\n",
    "    device,\n",
    "    use_custom_head=False,\n",
    "    pooler_out_features=768,  # Default: 768.\n",
    "    pooler_drop_prob=0.0,  # Default: 0.0\n",
    "    classifier_drop_prob=0.1,  # Default: 0.1\n",
    "    use_fast_tokenizer=False\n",
    ")\n",
    "\n",
    "deberta_with_annotator_heads_model = DebertaWithAnnotatorHeads(\n",
    "    deberta_encoder=deepcopy(deberta_model.deberta),\n",
    "    deberta_pooler=deepcopy(deberta_model.pooler),\n",
    "    deberta_dropout=deepcopy(deberta_model.dropout),\n",
    "    num_labels=num_labels,\n",
    "    annotator_ids=annotator_ids,\n",
    ")\n",
    "\n",
    "del deberta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37117cd8-49d8-4aec-b68a-445ba38e0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return deberta_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        # return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497966b6-ef34-4d58-9609-97f79a6ad6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 19:07:26,128 - sepheads_model_training_mhs - INFO - Creating tokenized datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd3f289279f42b9970e84a22b290a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be0d15bd5b74b8e87ef67a69636f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/21742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe9532dfee341daa3c5db07974d1eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create tokenized datasets.\n",
    "logger.info('Creating tokenized datasets')\n",
    "\n",
    "tokenized_training_data = (\n",
    "    # Create datast object from the DataFrame.\n",
    "    datasets.Dataset.from_dict(\n",
    "        training_data[[\n",
    "            'text',\n",
    "            'toxic_score',\n",
    "            'annotator_id'\n",
    "        ]].rename(\n",
    "            columns={\n",
    "                'toxic_score': 'label',\n",
    "                'annotator_id': 'annotator_ids',\n",
    "            }\n",
    "        )\n",
    "        .to_dict(orient='list')\n",
    "    )\n",
    "    # Tokenize.\n",
    "    .map(tokenize_function, batched=True)\n",
    "    # Remove useless column.\n",
    "    .remove_columns(\"text\")\n",
    "    .shuffle()\n",
    "    .flatten_indices()\n",
    ")\n",
    "\n",
    "tokenized_test_data = (\n",
    "    # Create datast object from the DataFrame.\n",
    "    datasets.Dataset.from_dict(\n",
    "        test_data[[\n",
    "            'text',\n",
    "            'toxic_score',\n",
    "            'annotator_id'\n",
    "        ]].rename(\n",
    "            columns={\n",
    "                'toxic_score': 'label',\n",
    "                'annotator_id': 'annotator_ids',\n",
    "            }\n",
    "        )\n",
    "        .to_dict(orient='list')\n",
    "    )\n",
    "    # Tokenize.\n",
    "    .map(tokenize_function, batched=True)\n",
    "    # Remove useless column.\n",
    "    .remove_columns(\"text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed9dbe3-d0de-43bc-a8cd-9847b1bdf392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_699003/1119375256.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_ID = 'sepheads_model_training_mhs_enlarged_dataset_1'\n",
    "MODEL_OUTPUT_DIR = f'/data1/moscato/personalised-hate-boundaries-data/models/mhs/{EXPERIMENT_ID}/'\n",
    "N_EPOCHS = 10\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Options: 'no', 'epoch', 'steps' (requires the `save_steps` argument to be set though).\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,  # Default: 8.\n",
    "    gradient_accumulation_steps=1,  # Default: 1.\n",
    "    per_device_eval_batch_size=32,  # Default: 8.\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.0,  # For linear warmup of learning rate.\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    # label_names=list(roberta_classifier.config.id2label.keys()),\n",
    "    logging_strategy='epoch',\n",
    "    logging_first_step=True,\n",
    "    logging_dir=f'../../tensorboard_logs/{EXPERIMENT_ID}/',\n",
    "    # logging_steps=10,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=deberta_tokenizer)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=deberta_with_annotator_heads_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=tokenized_test_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=deberta_tokenizer,\n",
    "    compute_metrics=compute_metrics_sklearn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4de3270-4874-43b3-8c00-fcaec018dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='6800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/6800 00:19 < 2:50:30, 0.66 it/s, Epoch 0.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving config file in the checkpoints directories\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m checkpoint_dir \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(MODEL_OUTPUT_DIR):\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:118\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 118\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/phb/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()\n",
    "\n",
    "logger.info('Saving config file in the checkpoints directories')\n",
    "\n",
    "for checkpoint_dir in os.listdir(MODEL_OUTPUT_DIR):\n",
    "    CHECKPOINT_PATH = os.path.join(MODEL_OUTPUT_DIR, checkpoint_dir)\n",
    "\n",
    "    config = DebertaWithAnnotatorHeadsPretrainedConfig(\n",
    "        num_labels=2,\n",
    "        annotator_ids=[int(aid) for aid in annotator_ids],\n",
    "        deberta_model_dir=model_dir\n",
    "    )\n",
    "    \n",
    "    config.save_pretrained(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c367ed-98f1-4d75-b6c0-5937b0cd2c0e",
   "metadata": {},
   "source": [
    "## Check: manually reproduce the metrics seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e06cd5c-5246-44ac-a712-7e05da2244c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_utils import send_batch_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d256807e-46f7-4dd5-b3a2-a614821354df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-1611',\n",
       " 'checkpoint-1790',\n",
       " 'checkpoint_1611_hatecheck_text_encodings.pkl',\n",
       " 'checkpoint_1611_hatecheck_predicted_logits.pkl',\n",
       " 'checkpoint_1611_hatecheck_predictions_catalog.csv',\n",
       " 'checkpoint_1611_hatecheck_predictions_annotators_data.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e42578-450e-423f-afef-4ac21757e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_steps = 1611\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(MODEL_OUTPUT_DIR, f'checkpoint-{checkpoint_steps}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9243dec-64d4-491a-b08d-68094bed8170",
   "metadata": {},
   "source": [
    "**Note:** if loading the model as a `DebertaWithAnnotatorHeadsPretrained` object (see the `use_pretrained_object` option below), in case the training did not save a config object (e.g. if a `DebertaWithAnnotatorHeads` was used instead of a `DebertaWithAnnotatorHeadsPretrained`), then the config must be created and placed in the checkpoint's directory. If we do this, we need to make sure that the annoatator IDs are the same as the ones we used for training (i.e. that the data above was loaded exactly as during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dddfd960-c0c8-419a-82b4-96081b185df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = DebertaWithAnnotatorHeadsPretrainedConfig(\n",
    "#     num_labels=2,\n",
    "#     annotator_ids=[int(aid) for aid in annotator_ids],\n",
    "#     deberta_model_dir=model_dir\n",
    "# )\n",
    "\n",
    "# config.save_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "# # config = DebertaWithAnnotatorHeadsPretrainedConfig.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424e1c3e-98a7-47a5-8861-cb110b196ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier_loaded = DebertaWithAnnotatorHeadsPretrained.from_pretrained(CHECKPOINT_PATH).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa8bd40-f6bd-45a3-9e11-1ea9f2eed744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33080c85c66c45d48900c8dc52d09cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_logits = []\n",
    "\n",
    "# Loop over test samples and accumulate the predictions.\n",
    "for row in tqdm(test_data.to_dict(orient='records')):\n",
    "    with torch.no_grad():\n",
    "        output = classifier_loaded(**send_batch_to_device(\n",
    "            dict(\n",
    "                **deberta_tokenizer(\n",
    "                    row['text'],\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ),\n",
    "                **{'annotator_ids': torch.LongTensor([row['annotator_id']]).to(device=device)}\n",
    "            ),\n",
    "            device,\n",
    "            return_batch=True\n",
    "        ))['logits']\n",
    "\n",
    "    predicted_logits.append(output)\n",
    "\n",
    "predicted_logits = torch.cat(predicted_logits).cpu().numpy()\n",
    "predicted_toxic_score = np.argmax(predicted_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b8eb0c5-66e8-454d-8782-fe2532d19482",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTED_LOGITS_OUTPUT_PATH = '/data1/moscato/personalised-hate-boundaries-data/models/mhs/sepheads_model_training_mhs_test_2/checkpoint_1611_mhs_predicted_logits.pkl'\n",
    "\n",
    "# with open(PREDICTED_LOGITS_OUTPUT_PATH, 'wb') as f:\n",
    "#     pickle.dump(predicted_logits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d8102dc-430e-43ca-8768-031e37dfa6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.92      0.82      1211\n",
      "           1       0.87      0.64      0.74      1046\n",
      "\n",
      "    accuracy                           0.79      2257\n",
      "   macro avg       0.81      0.78      0.78      2257\n",
      "weighted avg       0.80      0.79      0.78      2257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_pred=predicted_toxic_score,\n",
    "    y_true=test_data['toxic_score']\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
